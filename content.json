{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"history","text":"","link":"/history/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"AQS和ReentrantLock","text":"AQS子类继承于AQS的子类。 ReentrantLock在ReentrantLock中的lock方法123public void lock() &#123; sync.lock();&#125; 就是对Sync类的调用。 锁的获取大体的思路，就是先尝试获取一个锁，如果失败，说明锁正在被人占着，这时候需要把当前线程放到一个容器里，然后挂起。下面看看具体的实现是啥。 如果是非公平锁123456final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1);&#125; 在AQS类中有个state状态，用cas去改变他， 0代表空闲，1代表正在被占用。这个就相当于一个锁。1private volatile int state; 如果state=0的话，代表当前锁没有被任何线程锁起来。所以首先尝试获取这个锁，如果成功了，那么把当前的独占线程设为当前线程。 不然，调用acquire方法，加1。为什么加1呢，因为ReentantLock是可重入锁，同一个线程每一次获取都是加1，每一次释放就减1。下面我们会看到。 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; acquire方法在AQS中。但是tryAcquire是交给子类去实现的。 123protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125; 因为是非公平锁，所以调用nonfairTryAcquire。 123456789101112131415161718final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; 在nonfairTryAcquire中，先再次尝试获取一下state。如果失败，再判断下当前的独占线程是不是自己，如果是自己，就把acquire加上去。这里就是可重入的逻辑。 如果获取失败，而且当前独占线程也不是自己，那么返回false。再返回到AQS中的acquire逻辑，if中第一个条件失败了，那么执行第二个条件acquireQueued(addWaiter(Node.EXCLUSIVE), arg)，就是把当前线程丢到一个等待锁的容器中并挂起。 Node的mode有下面这几种123456789//独占static final Node EXCLUSIVE = null;//这个线程取消获取锁static final int CANCELLED = 1;static final int SIGNAL = -1;//这个线程在等待某个条件static final int CONDITION = -2;//节点唤醒需要向下传播，和读写锁有关static final int PROPAGATE = -3; 上面调用的是独占的EXCLUSIVE。 123456789101112131415private transient volatile Node tail;private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; 看到node应该可以猜到这个放等待线程的容器是链表。先cas尝试一下把当前线程包装的node放到末尾。如果失败进入enq()中。123456789101112131415private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; enq的逻辑就是不断自旋，cas加到末尾。其中也包含了head未初始化的情况。 好，现在成功加到末尾，调用acquireQueued方法。在这里进行挂起之类的操作。123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; node.predecessor得到的是节点的prev节点。这里假设当前节点前面有很多节点，那么第一个条件肯定不满足，进入到第二个if中。1234567891011121314151617181920private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws &gt; 0) &#123; //表示前一个线程已经取消获取锁 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; //如果状态为0或者为PROPAGATE compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; 判断前一个节点的waitStatus。如果是SIGNAL，返回true如果大于0，那就是cancelled了，那就跳过不然就是cas把当前的pred的node的waitStatus设为SIGNAL 如果成功1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 那么parkAndCheckInterrupt就把当前线程挂起。 释放锁在unLock中12345678910111213141516171819202122232425262728public void unlock() &#123; sync.release(1);&#125; protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; tryRelease方法，就是把state减掉，然后把当前的独占线程置空。在回到release方法，那个unparkSuccessor调用。 1234567891011121314151617181920212223242526private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; 可以看到这里就是唤醒了下一个node节点中的等待线程。 那么ReentrantLock的公平和非公平体现在哪里呢。我一开始以为的是head节点会唤醒所有的后缀节点，结果不是。已经排队的节点还是按照排队顺序来唤醒。不过在头节点唤醒时，可以有其他的线程来插队。再回到lock方法123456final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1);&#125; 在获取失败，加到队列的过程中，也尝试了两次插队，最后都失败了，才加入到队列中。 另外，AQS的等待队列只是FIFO的，也就是说不支持优先权。 总结AQS为独占和共享提供了基本的接口，需要我们自己去实现tryAcquire等方法。 获取锁正常来说，如果tryAcquire失败，那么AQS就会把当前线程包在Node中，加到等待队列中。等待队列是一个FIFO的双向链表。如果是独占的锁，那么就很简单，ReentrantLock就是借助一个int的变量表明被占用还是空闲。如果是共享锁，这就要看具体的情况了，在CountDownLatch中，就是看当前的钥匙是不是0，如果不是0，说明还是有线程没有countdown，那就加到等待队列去。在读写锁中，如果当前有读锁或者写锁，那就失败。 释放锁然后每次head节点在临界区执行完成，就会选择唤醒下一个节点。如果是独占的，就唤醒下一个，如果是共享的，那么就唤醒下一波所有的要获取共享锁的线程。","link":"/2018/02/24/AQS和ReentrantLock/"},{"title":"ArrayList源码分析","text":"前言12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 继承结构是这样 数据结构与扩容操作既然名字是ArrayList，那么底层多多少少和数组有关。 先来看看构造函数里做了啥。12345678910111213141516transient Object[] elementData;private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125;&#125; 当构造函数的参数为空时，给成员变量elementData赋了一个空的数组。当我们指定了初始大小的时候，如果参数大于0，那么elementData就是我们设的大小，如果等于0，那么还是赋了一个空的数组给elementData。 进行add操作时 。12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); elementData[size++] = e; return true;&#125; 我们可以注意到一个ensureCapacityInternal的函数，这个函数是确保底层的数组有足够的空间进行增加的。 12345678private static final int DEFAULT_CAPACITY = 10;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125; 在ArrayList中还有一个成员变量是DEFAULT_CAPACITY，默认大小是10。如果是无参数的new的ArrayList话而且是第一次调用add的话，那么minCapacity就会是10。但是如果是下面再进行add的话，再进去这个函数，则是直接调用ensureExplicitCapacity函数。不会在进行Math.max的操作了。这里还是没有真正就行底层扩容操作。 再来看看ensureExplicitCapacity函数12345678protected transient int modCount = 0;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125; 这里用到了modCount变量，这个在HashMap中也有看到，这个和fail-fast有关，下面会介绍。下面还对minCapacity - elementData.length &gt; 0进行了一次判断，因为现在的长度的数组可能还没用完，如果这样的话，那么不用扩容，直接用就行了。还有一种情况就是minCapacity已经超过int最大值了，变成了负数了，那样也不会进行扩容。 关键是在grow函数中123456789101112private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 这里才看到了真正的扩容操作。但是别急，在操作之前还需要进行判断作为保证。这里有三个变量，和名字一样， minCapacity就是这次扩容后的最小容量 newCapacity是最后进行扩容的大小，也就是最终扩容时用到的 oldCapacity是现在的大小。 现在我们在回去看add参数的话，会看到他调用的ensureCapacityInternal时的参数是当前的大小加1，注意，只是加了1而已，每次add都扩容的话，那么代价肯定会非常高的。 这里的溢出检查的newCapacity是现在的size加上size的一半。也就是原来的大小的3/2。所以不出意外的话，大多数情况下，每次扩容的大小都是原来的大小的3/2。那么我们还需要考虑特殊情况，就是溢出的情况。 如果newCapacity乘上了原来的3/2，超过了int的范围，变成了负数，那么就是newCapacity - minCapacity &lt; 0为true了，那么newCapacity的大小就是minCapacity，也就是原来的大小加上了1。 如果没有溢出，但是比MAX_ARRAY_SIZE大，也就是在Integer.MAX_VALUE - 9 到Integer.MAX_VALUE之间。那么newCapacity的大小就是hugeCapacity的返回值。 12345678private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 这里如果minCapacity溢出了，也就是现在的size的大小已经是Integer.MAX_VALUE了，那么就抛出OutOfMemoryError了。 在hugeCapacity函数中，minCapacity和MAX_ARRAY_SIZE进行了比较，如果minCapacity大于MAX_ARRAY_SIZE但是还没有溢出，那么就直接扩容到Integer.MAX_VALUE。 看到这里我猜想了如果出现了极端的情况，就是add时size刚好是Integer的最大值会怎么样？那时，minCapacity - elementData.length的结果是1，还是可以进入grow函数。最后进入hugeCapacity函数，最后抛出OutOfMemoryError。给力。代码的鲁棒性很好。 fail-fast机制与遍历上面我们提到一个变量modCount，每次我们进行add或者是进行一些其他的修改数组的操作时，这个参数就会加1。在单线程中，这个当然是不会出错的。但是在多线程中，我们知道如果我们在遍历时，可能会抛出ConcurrentModificationException。这个异常就是提醒我们在遍历时，有其他的线程对数组进行了修改。 1234567891011121314@Overridepublic void forEach(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); final int expectedModCount = modCount; @SuppressWarnings(\"unchecked\") final E[] elementData = (E[]) this.elementData; final int size = this.size; for (int i=0; modCount == expectedModCount &amp;&amp; i &lt; size; i++) &#123; action.accept(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125; 比如这个forEach方法，在进行操作前先记录当前的modCount大小，最后操作完比对现在的modCount，如果不一样，那么就抛出异常。 RandomAccess接口这个接口是空的，只是做一个标记的作用。12public interface RandomAccess &#123;&#125; 什么标记呢，就是对实现了这类接口的类进行遍历时，12for (int i=0, n=list.size(); i &amp;lt; n; i++) list.get(i); 12for (Iterator i=list.iterator(); i.hasNext(); ) i.next(); 如果第一种遍历方式要比第二种快，那么他就应该实现RandomAccess接口，这样我们在使用的时候，在特定场景可能会根据是否实现了RandomAccess来进行遍历。这个我们在比较LinkedList的时候就能理解了。因为LinkedList是链表的方式，如果我们使用第一种遍历，那相当于每次get都从头开始找了。所以LinkedList就不能实现这个接口。而ArrayList底层是数组，所以直接get反而更快一点，而用第二种还需要进行一个SubList的构造。 其他12345678public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125;&#125; 这个方法很有意思，因为我们的扩容机制不是一个一个的增加的，一下子增加原来的3/2大小。如果我们不再往里面增加元素的话，那么多出来的就很浪费。这个方法就是去除多余的没设置值的空间。 1234567891011public Object clone() &#123; try &#123; ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; // this shouldn't happen, since we are Cloneable throw new InternalError(e); &#125;&#125; clone方法可以看到，做的是浅拷贝。 1234567891011121314public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 这个方法，可以看到开销还是挺大的，那个注释很有意思，把后面缩小之后的引用置位null。如果不置位null，仅仅是size-1的话，那么位于size位的对象无法被GC收集。这样就会造成内存泄漏。 123456789public void clear() &#123; modCount++; // clear to let GC do its work for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0;&#125; clear方法并没有释放底层的数组，只是把每个引用置位null。","link":"/2017/12/07/ArrayList源码分析/"},{"title":"HashSet源码解析","text":"前言HashSet其实实现很简单，也很取巧。 底层结构123456private transient HashMap&lt;E,Object&gt; map; private static final Object PRESENT = new Object();public HashSet() &#123; map = new HashMap&lt;&gt;();&#125; 看构造函数就知道里面存了一个HashMap，而那个PRESENT的Object的作用就是给每个存进Set的对象的value。 基本方法123456789101112public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125;public boolean remove(Object o) &#123; return map.remove(o)==PRESENT;&#125;public boolean contains(Object o) &#123; return map.containsKey(o);&#125;public Iterator&lt;E&gt; iterator() &#123; return map.keySet().iterator();&#125; 这三个基本方法可以看出，其实HashSet就是封装了一个HashMap，每次存一个对象其实就是在Map中放了一个key -&gt; Object， 那个Object是共享的。而iterator方法返回的是keySet的iterator。​","link":"/2017/12/09/HashSet源码分析/"},{"title":"Java内部类","text":"前言最近看《Java编程思想》，看了内部类的讲解，发现自己好多都没听过或者没注意到。 指向外部类的指针每个内部类都有一个隐藏的外部类的this指针，可以通过Outer.this获得 12345678910111213141516171819public class Main &#123; private String message = \"hello\"; public String getMessage() &#123; return message; &#125; class Inner &#123; public Inner() &#123; System.out.println(Main.this.getMessage()); System.out.println(getMessage()); &#125; &#125; public static void main(String[] args) &#123; Main main = new Main(); Main.Inner inner = main.new Inner(); &#125;&#125; 这个例子看其实意义不大，因为函数getMessage()没有歧义。如果Inner中也有个getMessage()，那么调用的就是Inner中的，这里this指针就派上用场了。 题外话，在《Java并发编程实战》33页页提到了这个隐藏指针，如果把对象逸出，那么可能出现为构造未完全就调用了方法。比如下面（抱歉写的有点臃肿）。。。 1234567891011121314151617181920212223242526272829303132333435363738interface Escape&#123; void doSome(Some some);&#125;class Some &#123; protected Demo demo; public void printI() &#123; demo.printI(); &#125;&#125;class Demo &#123; private int i = 0; public Demo(Escape escape) &#123; escape.doSome(new Some() &#123; &#123; this.demo = Demo.this; &#125; &#125;); i = 1; &#125; public void printI() &#123; System.out.println(i); &#125; &#125;public class Main &#123; public static void main(String[] args) &#123; Escape escape = new Escape() &#123; @Override public void doSome(Some some) &#123; some.printI(); &#125; &#125;; Demo demo = new Demo(escape); &#125;&#125; Demo构造完应该i = 1但是这里调用过之后却是输出了0这就是内部类是对象隐藏指针逸出导致得到了未构造完全的对象。 返回private内部类正常类的来说，只能是public或者缺省的，不可以是private的。但是内部类可以是public，priavte， protected或者缺省的。 12345678910111213class Demo &#123; private class Innter &#123;&#125; public Innter getInnerInstence() &#123; return new Innter(); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; Demo demo = new Demo(); //下面是不可以的，因为Inner是private的 //Demo.Inner inner = demo.getInnerInstence(); &#125;&#125; 作为一种折中的方法，我们可以申明一个接口来解决这个问题 12345678910111213interface ForTest&#123;&#125;class Demo &#123; private class Innter implements ForTest &#123;&#125; public Innter getInnerInstence() &#123; return new Innter(); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; Demo demo = new Demo(); ForTest inner = demo.getInnerInstence(); &#125;&#125; 普通内部类不能有static方法和实例变量1234567public class Main &#123; class Inner &#123; //Inner classes cannot have static declarations //public static String message = \"hello\"; &#125;&#125; 静态内部类如果你想要内部类可以用static，那么可以使用静态内部类。但是静态内部类没有对外部类的引用了。就像是完全的独立的一个类。创建静态内部类的时候也不需要先创建外部类了。 123456789101112public class Main &#123; private String name = \"hello\"; static class Inner &#123; public static String message = \"hello\"; public Inner() &#123; //Non-static field 'name' cannot be referenced from a static context //System.out.println(name); &#125; &#125;&#125; 匿名类的构造匿名类因为没有类名，所以构造函数没法重新定义，不过可以用{}包裹起来作为构造函数。 123456789101112131415161718public class Main &#123; private String name = \"hello\"; public Main(String name) &#123; this.name = name; &#125; public Main getMain(String name) &#123; return new Main(name)&#123; //报错 //public Main()&#123;&#125; &#123; System.out.println(\"name is\" + name); &#125; &#125;; &#125;&#125; 内部类的构造参数如果想要在内部类中使用其他的对象，必须是final的或者effectively final的，也就是说不允许内部类修改外界的对象。什么是effectively final呢，就是如果你在内部类内部没有修改这个参数，那么jvm就默认这个是final的，不需要显式的加final关键词了。是在Java8中引入的，有点像语法糖。 1234567891011121314151617public class Main &#123; private String name = \"hello\"; public Main(String name) &#123; this.name = name; &#125; public Main getMain(String name) &#123; return new Main(name)&#123; &#123; //variable 'name' is accessed from within inner class, need to be final or effectively final name = \"lll\"; &#125; &#125;; &#125; &#125; 显然我们想知道为什么呢？stackoverflow的答案 接口中的内部类接口中定义的方法默认是public的，接口中定义的类默认是static的。 静态内部类作为测试一般我们会为每个类做一个main方法进行一些简单的测试。但是这样编译过之后，class文件中还是有main方法的，占用了空间。我们可以使用静态内部类作为测试，因为编译过后静态内部类会生成一个单独的class文件，生产环境中可以不把它包含进去。 我觉得没啥用，因为不止静态内部类，普通内部类和匿名类也会单独生成一个class文件。生产环境中一个一个删除不是傻逼吗。。。 为什么要引入内部类作为不能多继承的补充好有道理 继承内部类因为内部类拥有外部类的引用，所以继承的时候要有点技巧。 123456789101112131415class Demo &#123; class Inner&#123;&#125;&#125; public class Main extends Demo.Inner&#123; //构造函数只能这么写。。。 public Main(Demo demo) &#123; demo.super(); &#125; public static void main(String[] args) &#123; &#125;&#125; 内部类重写内部类可以向方法一样重写吗并进行多态吗不行","link":"/2017/08/05/Java内部类/"},{"title":"ConcurrentHashMap源码分析","text":"前言ConcurrentHashMap的源码长的让人惊叹。我在idea打开的1.8版本达到了6000多行。 ConcurrentHashMap是多线程安全的HashMap，但是和HashTable的简单的加上同步的方法不同，他运用了很多高级复杂的方法来进行同步，很多都是避免的昂贵的锁操作。 底层其实有很多和HashMap相同的地方，比如处理Hash冲突都是链表和红黑树。树化和链表化的阈值都是一样的。 简单的说有下面几个重点： Unsafe的运用，就是CAS操作 每个bin为一把锁 红黑树的读写分离 12public class ConcurrentHashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements ConcurrentMap&lt;K,V&gt;, Serializable ConcurrentMapConcureentHashMap实现了ConcurrentMap接口，这个接口的方法全部来自Map。看起来只是一个标记作用，但是其实不是。比如在Map中1234567default V putIfAbsent(K key, V value) &#123; V v = get(key); if (v == null) &#123; v = put(key, value); &#125; return v;&#125; putIfAbsent方法被设为default，只要implements了这个接口就自动获得这个方法。但是在ConcurrentMap中，1V putIfAbsent(K key, V value); 去掉了default，这样在ConcurrentHashMap就必须自己实现了。ConcurrentHashMap的实现都是原子的。 阈值参数12345678910111213141516171819202122232425262728293031323334353637//数组的最大长度，就是桶的数量，下面简称bin了private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //如果我们没有指定初始的bin的大小，默认是16，2的4次方private static final int DEFAULT_CAPACITY = 16; //和toArray及相关的方法有关，最大的Array大小static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; //和之前的版本的分段锁有关，这个我们最后再谈，这个保留这个只是为了保持兼容性private static final int DEFAULT_CONCURRENCY_LEVEL = 16; //默认的负载因子，当桶被填了超过百分之75时，会resize，增加桶的数量//换句话说，当空的bin少于百分之25时，会增加bin的数量private static final float LOAD_FACTOR = 0.75f; //当bin中链表的数量超过8时，会变成红黑树static final int TREEIFY_THRESHOLD = 8; //当树中节点少于6时，会变成链表。static final int UNTREEIFY_THRESHOLD = 6; //进行树化的另一个条件，就是需要bin的数量达到64static final int MIN_TREEIFY_CAPACITY = 64; private static final int MIN_TRANSFER_STRIDE = 16;private static int RESIZE_STAMP_BITS = 16; //help resize的最大线程数private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1; private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS; //在扩容时的状态static final int MOVED = -1; static final int TREEBIN = -2; static final int RESERVED = -3; Node类链表Node123456789101112131415161718192021222324252627282930313233//为了简化删了一些，重点是这个类的setValue方法，是不支持的。//这个是链表节点的类，而树节点的类在下面。static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + \"=\" + val; &#125; public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125;&#125; 红黑树节点1234567891011121314151617181920212223242526//这个是红黑树的节点类，继承了Node类。static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; return findTreeNode(h, k, null); &#125; /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ final TreeNode&lt;K,V&gt; findTreeNode(int h, Object k, Class&lt;?&gt; kc) &#123; //... &#125;&#125; 红黑树Bin12345678910111213141516171819//这个是红黑树的一个抽象。也是继承了Node类，//所以在进行一些操作的时候需要用instanceof进行一些判断。//我们一开始说ConcurrentHashMap和之前的分段锁不同，//他每个bin都是一把锁，所以在TreeBin中有一些lock函数//这个类还自带读写锁属性，因为ConcurrentHashMap的读是没有给头Node进行同步//而是直接调用的红黑树的查找方法//如果是链表的话，不加锁，但是如果是红黑树的话，会加上读锁//线程得到写锁的时候必须先等读锁释放。static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock //TreeBin的put，get，remove基本操作。就是红黑树的操作。&#125; ForwardingNode12345678910111213141516171819202122232425262728293031323334353637//这是一个特殊的Node，当table正在transfer的时候，会被切到原table的bin的头结点去//用于连接原table和nextTable//它包含了nextTable，hash值为-1，其他的都是null。//在并发transfer的时候，如果发现这个节点是ForwardingNode，那就说明这个节点的转移已经完成了。static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; for (;;) &#123; int eh; K ek; if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; if (eh &lt; 0) &#123; if (e instanceof ForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; &#125; else return e.find(h, k); &#125; if ((e = e.next) == null) return null; &#125; &#125; &#125;&#125; 重要的成员变量 1234567891011121314151617181920212223242526272829303132//cpu的个数static final int NCPU = Runtime.getRuntime().availableProcessors(); //bin数组，在第一次插入操作进行初始化transient volatile Node&lt;K,V&gt;[] table; //扩容时的数组private transient volatile Node&lt;K,V&gt;[] nextTable; //当前键值对总个数private transient volatile long baseCount; //这个比较重要，是控制标识符，源码中叫Table initialization and resizing control//就是进行初始化和扩容的控制。//当是负数时，table正在初始化或者扩容//-1代表正在初始化 ,-N 表示有N-1个线程正在进行扩容操作//当table是null时，如果这个值大于0，那就是tablede初始化大小，如果等于0，那就使用默认大小//当table不是null时，等于下一次resize时node需要达到的数量，就是容量乘以负载因子。private transient volatile int sizeCtl;private transient volatile int transferIndex; //自旋锁private transient volatile int cellsBusy; //这个与size有关private transient volatile CounterCell[] counterCells; //下面这三个和迭代有关private transient KeySetView&lt;K,V&gt; keySet;private transient ValuesView&lt;K,V&gt; values;private transient EntrySetView&lt;K,V&gt; entrySet; unsafe1234567891011121314151617181920212223242526272829303132333435363738// Unsafe mechanicsprivate static final sun.misc.Unsafe U;private static final long SIZECTL;private static final long TRANSFERINDEX;private static final long BASECOUNT;private static final long CELLSBUSY;private static final long CELLVALUE;private static final long ABASE;private static final int ASHIFT;static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField(\"sizeCtl\")); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField(\"transferIndex\")); BASECOUNT = U.objectFieldOffset (k.getDeclaredField(\"baseCount\")); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField(\"cellsBusy\")); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField(\"value\")); Class&lt;?&gt; ak = Node[].class; //可以获取数组第一个元素的偏移地址 ABASE = U.arrayBaseOffset(ak); //arrayIndexScale可以获取数组的转换因子，也就是数组中元素的增量地址 //将arrayBaseOffset与arrayIndexScale配合使用，可以定位数组中每个元素在内存中的位置。 int scale = U.arrayIndexScale(ak); if ((scale &amp; (scale - 1)) != 0) throw new Error(\"data type scale not a power of two\"); ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125; 辅助方法12345678910111213141516171819202122232425262728293031323334353637//让hash值分布的更均匀，让原本的hash值前16位和后16位取异或在和HASH_BITS取与。static final int HASH_BITS = 0x7fffffff;static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125;//找到与c最接近的2的整数次方的那个数，用的方法很巧妙。private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125;//得到tab第i个位置的Node，用了unsafe，很玄幻的操作。//为什么不直接用tab[i]返回呢//因为在java内存模型中，我们已经知道每个线程都有一个工作内存，里面存储着table的副本//虽然table是volatile修饰的，但不能保证线程每次都拿到table中的最新元素//Unsafe.getObjectVolatile可以直接获取指定内存的数据，保证了每次拿到数据都是最新的。//来自参考的第一篇文章。//这里的ABASE指的是一个元素的长度//i &lt;&lt; ASHIFT计算的是第i个元素和第一个的偏移Class&lt;?&gt; ak = Node[].class;ABASE = U.arrayBaseOffset(ak);static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; //原理同上。static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; 树化操作123456789101112131415161718192021222324252627282930//将index位置的tab树化//但是如果table小于64，而直接扩容//运用了同步，所以在多线程中是安全的。private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; //如果table的长度小于64，则不转树，而进行扩容到两倍 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) tryPresize(n &lt;&lt; 1); //不然就进行转树操作。 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; 树退化为链表 这个操作只有在transfer时，发现红黑树节点少于6，才会进行。123456789101112static &lt;K,V&gt; Node&lt;K,V&gt; untreeify(Node&lt;K,V&gt; b) &#123; Node&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; q = b; q != null; q = q.next) &#123; Node&lt;K,V&gt; p = new Node&lt;K,V&gt;(q.hash, q.key, q.val, null); if (tl == null) hd = p; else tl.next = p; tl = p; &#125; return hd;&#125; 初始化方法 构造函数123456789101112131415161718192021222324252627282930313233//空的，使用默认值public ConcurrentHashMap() &#123; &#125;//自定义初始化的容量大小，就是bin的数量。//其中的tableSizeFor方法在HashMap中是一样的，为了保证容量是2的整数次方//会找到最接近initialCapacity的那个2的整数次方数。public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;&#125; //还可以自定义负载因子，但是并不能改变系统的负载因子，仅仅是用在构造函数中。public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1);&#125;//自定义负载因子，初始容量，并发线程数，concurrencyLevel的是并发更新map的线程数//就是1.8之前的实现的分段锁的个数，现在已经不用，在这儿仅仅为了保持兼容性。public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (initialCapacity &lt; concurrencyLevel) // Use at least as many bins initialCapacity = concurrencyLevel; // as estimated threads long size = (long)(1.0 + (long)initialCapacity / loadFactor); int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); this.sizeCtl = cap; &#125; table初始化1234567891011121314151617181920212223242526272829//table的初始化，这里需要考虑多线程竞争。private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; //如果已经有线程正在进行初始化，那么调用一下yield进入自旋等待初始化结束 //这里的yield其实取决于操作系统，作用是提醒操作系统我这个线程主动让出时间片。 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin //cas操作，只有一个线程进入初始化，通过cas设为-1。 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; //这个相当于sc = n*0.75，也就是乘上了负载因子0.75 //不过这种写法也真的是服。。。。 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 基本操作put方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public V put(K key, V value) &#123; return putVal(key, value, false);&#125; //第三个参数是是否只有在没有的情况进行putfinal V putVal(K key, V value, boolean onlyIfAbsent) &#123; //key和value都不允许null if (key == null || value == null) throw new NullPointerException(); //得到key的hash值，spread函数让他分布的更均匀点。 int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果table还没有进行初始化，进行初始化。 if (tab == null || (n = tab.length) == 0) tab = initTable(); //如果tab[i] //的位置还是空的，就进行cas操作set进去 //如果cas成功，就put成功了，就直接退出循环。 //如果失败，那就进行自旋，再次尝试插入。 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; &#125; //如果发现为-1，说明有其他线程正在进行扩容操作，就不自旋了 //一起去参加扩容把，关于扩容参见下面 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; //准备进行同步方法，和分段不同的是 //这里会给tab[i]的元素进行加锁。相当于一个bin一个锁。 V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; //如果fh大于0，说明这个bin是链表 if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //不然就是红黑树，调用TreeBin的put方法。 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //如果链表长度大于阈值，就进行树化 //但是其实进去treeifyBin就会看到，还需要bin的数量大于64 //不然就是直接扩容 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //如果插入成功，就总数加一，判断要不要进行扩容 addCount(1L, binCount); return null;&#125; get方法123456789101112131415161718192021public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //得到hash值 int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //说明正在进行扩容操作，node是forwardingNode，调用forwardingNode的find方法。 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; remove123456789101112131415161718192021222324252627282930313233public V remove(Object key) &#123; return replaceNode(key, null, null);&#125; final V replaceNode(Object key, V value, Object cv) &#123; int hash = spread(key.hashCode()); for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; //如果正在进行迁移，那么就加入transfer行列。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; boolean validated = false; //否则就对table[i]进行加锁 synchronized (f) &#123; //...省略了，和get的方法差不多 &#125; if (validated) &#123; if (oldVal != null) &#123; if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null;&#125; 扩容操作扩容操作分为两步 初始化nextTable数组，这个由单线程完成 迁移table中的数据到nextTable中，这个可以并发的进行主要操作是在transfer中，transfer方法可能被下面三个函数调用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123//扩容到sizeprivate final void tryPresize(int size) &#123; int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; //sizeCtl&gt;=0说明没有正在扩容，或者表还没初始化 while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; //如果表还没初始化 if (tab == null || (n = tab.length) == 0) &#123; //扩容的值取两者的较大的 n = (sc &gt; c) ? sc : c; //进行cas操作 if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; //成功，进行初始化 if (table == tab) &#123; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; &#125; &#125; //如果现在的容量已经到达最大值或者扩容值比现在的小，就直接退出 else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; //已经进行过初始化 else if (tab == table) &#123; int rs = resizeStamp(n); //如果sc小于0，说明正在进行扩容 //不过这个可能小于0么，，我看了看好像不太可能啊。 if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //这里才是真正的进行扩容操作。 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); &#125; &#125;&#125;static final int resizeStamp(int n) &#123; return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));&#125;//检查并进行扩容操作。//首先初始化nextTable，大小为原来table大小的两倍//把当前的size加上x，然后检查是否要进行扩容。//这里涉及到size的计算，这个我们下面会讲。private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); //如果sc小于0，说明正在进行扩容操作，而加入进去进行扩容。 if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //不然就进行扩容，同nextTable中传了一个null进去 //在transfer中会检查，如果是null就会先new一下nextTable。 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125;//因为进行扩容时，需要把原来的元素搬到nextTable中，这个过程可以各个线程协助完成。//如果在put操作中发现table正在扩容，则当前线程就会加入help转移的队伍中来。final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; Node&lt;K,V&gt;[] nextTab; int sc; if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; int rs = resizeStamp(tab.length); while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; //这里把通过cas把sc+1，如果成功，就加入transfer行列。 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125; 其中并发迁移操作又分为几步： 首先从table的最后一个节点开始找，如果table[i]是空，则通过cas把一个ForwardingNode放进去，标志已经迁移结束 如果table[i]是forwardingNode，则继续向前找 如果table[i]不为空，那么就对这个点进行加锁，把这个点的节点放到nextTable的i和i+n中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154//这个是扩容的转移函数，也是最重要的部分，上面的函数都对这个进行了调用。//其中transferIndex标记了在transfer中之前的table长度。private transient volatile int transferIndex;private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; //这里的判断为什么没有加锁呢，在多线程情况下看起来好像会造成错误 //我看了下这个函数的调用，其实就是上面几个，上面几个调用的时候 //nextTab肯定是已经进行了初始化的 //除了addCount中，那个也进行了cas进行保护。 if (nextTab == null) &#123; try &#123; @SuppressWarnings(\"unchecked\") //构造一个nextTable，他的容量是原来的两倍 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; //构造一个ForwardingNode，用于标记当前的bin已经被转移结束了。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); //标记是否继续往下个节点找的标志 boolean advance = true; //表示是否迁移结束的标志。 boolean finishing = false; for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; //找到了一个，把advance置空，准备进行迁移操作。 i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //如果节点为空，那么把forwardingNode放进去，标记为已经迁移结束。 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; else &#123; //把table[i]进行加锁。 //这边还对链表进行遍历,有点对半拆分的感觉 //把链表分表拆分为，hash&amp;n等于0和不等于0的，然后分别放在新表的i和i+n位置 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //如果是一个红黑树bin else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; //这里会判断红黑树的节点是否小于UNTREEIFY_THRESHOLD，也就是6 //如果小于，那么就是退化为链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; size值的计算这个是继承自AbstractMap的方法，问题是返回的是int，但是里面可能存在的值可能会大于int123456public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125; 所以在1.8中增加了这个方法，并且从注释上可以看到作者希望我们应该调用这个方法1234public long mappingCount() &#123; long n = sumCount(); return (n &lt; 0L) ? 0L : n; // ignore transient negative values&#125; 他们都调用了sumCount()方法，这个就是把baseCount加上所有的as的值。12345678910111213141516@sun.misc.Contended static final class CounterCell &#123; volatile long value; CounterCell(long x) &#123; value = x; &#125;&#125;private transient volatile CounterCell[] counterCells;final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125; 那为什么要这么做呢，这个回到上面的addCount方法看看，在进行更新的时候，会尝试使用cas更新baseCount但是如果更新失败，那么就尝试在counterCells上做文章，这里做的几乎和LongAdder一样，就不赘述了。12345678910111213141516171819private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; //...&#125; 总结有很多细节其实挺让我震惊的。 Node中如果可能是多线程访问的，都加了volatile进行修饰 锲而不舍的追求高效率的位运算，连乘0.75都换成n - n &gt;&gt;&gt; 2来代替 参考http://blog.csdn.net/lsgqjh/article/details/54867107","link":"/2017/12/18/ConcurrentHashMap源码分析/"},{"title":"HashMap源码解析","text":"Hash数据结构设计为什么HashMap的capacity是2的整数次幂？JDK1.8相对于1.7改进了什么？HashMap在多线程情况下会发生什么？ 散列函数Java中把每个对象散列成一个整数的方法是hashCode()，这个方法是从Object而来的。默认的是通过对象的内存地址来。所以几乎是唯一的，不会产生碰撞的。但是我们也可以自己选择重写这个方法。不过我们在重写hashCode方法时，别忘了也要重写equals方法。要保证hashCode和equals方法的行为一致。也就是说equals为true时，hashCode必然时相等的。equals为false时，hashCode可以相等也可以不相等。 碰撞冲突如果两个键的hashCode相同，那就是产生了碰撞。有很多方法解决碰撞，常见的是拉链法和线性探测法。 拉链法就是开一个Node的数组，然后每个元素指向一个链表。但hash值相同时，就存在对应的元素的单链表中。实现也较为简单。 借助这个再解释下hashCode和equals方法。hashCode产生的值用来寻找数组的索引，但是可能产生碰撞，于是equals方法就用来便利单链表找到相等的键。 线性探测法就是开一个Node数组，如果hash值相同，那么找下一个空位。怎么找下一个空位呢。最简单的就是每次+1，当然你也可以约定+2，+3。。。 HashMap源码解析HashMap用的是拉链法，但是1.8中还增加了改进。我们约定前面的数组的每一格称为桶约定桶后面存放的每一个数据称为bin 1.8中补充了红黑树，如果一个桶的bin数大于8，那么就把这个桶的链表转换为红黑树。 Map声明 12public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable HashMap继承自AbstractMap， 实现了Map接口的方法。 1public abstract class AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; AbstractMap同样实现了Map接口的方法。很多人可能奇怪为什么HashMap为什么不直接实现Map接口就好，中间要多个AbstractMap类呢。因为类库开发者考虑到有些人可能要实现自己的Map类，为了给降低他们的开发难度，于是开发了AbstractMap类。这样我们如果要实现自己的Map类，就继承AbstractMap类就好了。 Map接口的所有方法如下，其中有很多default方法，是Java8的新特性。有兴趣的可以百度一下。这里就不赘述了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229public interface Map&lt;K,V&gt; &#123; //Map具有特征的方法，看名字就大概就能猜到用处 int size(); boolean isEmpty(); boolean containsKey(Object key); boolean containsValue(Object value); V get(Object key); V put(K key, V value); V remove(Object key); void putAll(Map&lt;? extends K, ? extends V&gt; m); void clear(); Set&lt;K&gt; keySet(); Collection&lt;V&gt; values(); //Entry见下 Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet(); //来自Object的方法 boolean equals(Object o); int hashCode(); //底层的数据单位Entry interface Entry&lt;K,V&gt; &#123; K getKey(); V getValue(); V setValue(V value); //来自Object boolean equals(Object o); int hashCode(); //key的比较，lambda表达式还能这么用。。。 public static &lt;K extends Comparable&lt;? super K&gt;, V&gt; Comparator&lt;Map.Entry&lt;K,V&gt;&gt; comparingByKey() &#123; return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; c1.getKey().compareTo(c2.getKey()); &#125; //value的比较 public static &lt;K, V extends Comparable&lt;? super V&gt;&gt; Comparator&lt;Map.Entry&lt;K,V&gt;&gt; comparingByValue() &#123; return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; c1.getValue().compareTo(c2.getValue()); &#125; //自定义key的比较 public static &lt;K, V&gt; Comparator&lt;Map.Entry&lt;K, V&gt;&gt; comparingByKey(Comparator&lt;? super K&gt; cmp) &#123; Objects.requireNonNull(cmp); return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; cmp.compare(c1.getKey(), c2.getKey()); &#125; //自定义value的比较 public static &lt;K, V&gt; Comparator&lt;Map.Entry&lt;K, V&gt;&gt; comparingByValue(Comparator&lt;? super V&gt; cmp) &#123; Objects.requireNonNull(cmp); return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; cmp.compare(c1.getValue(), c2.getValue()); &#125; &#125; //给定一个key获取value，同时指定一个default值，如果不存在就返回default值 default V getOrDefault(Object key, V defaultValue) &#123; V v; return (((v = get(key)) != null) || containsKey(key)) ? v : defaultValue; &#125; //对Map中每个键值对进行指定的action操作 //这个方法会抛出异常如果在遍历过程中某个Entry被删除了 default void forEach(BiConsumer&lt;? super K, ? super V&gt; action) &#123; Objects.requireNonNull(action); for (Map.Entry&lt;K, V&gt; entry : entrySet()) &#123; K k; V v; try &#123; k = entry.getKey(); v = entry.getValue(); &#125; catch(IllegalStateException ise) &#123; // this usually means the entry is no longer in the map. throw new ConcurrentModificationException(ise); &#125; action.accept(k, v); &#125; &#125; //把所有的value进行重新赋值，也会抛出异常 default void replaceAll(BiFunction&lt;? super K, ? super V, ? extends V&gt; function) &#123; Objects.requireNonNull(function); for (Map.Entry&lt;K, V&gt; entry : entrySet()) &#123; K k; V v; try &#123; k = entry.getKey(); v = entry.getValue(); &#125; catch(IllegalStateException ise) &#123; // this usually means the entry is no longer in the map. throw new ConcurrentModificationException(ise); &#125; // ise thrown from function is not a cme. v = function.apply(k, v); try &#123; entry.setValue(v); &#125; catch(IllegalStateException ise) &#123; // this usually means the entry is no longer in the map. throw new ConcurrentModificationException(ise); &#125; &#125; &#125; //如果key不存在，则put进去 default V putIfAbsent(K key, V value) &#123; V v = get(key); if (v == null) &#123; v = put(key, value); &#125; return v; &#125; //移除一个键值对，同时必须键值对都相等 default boolean remove(Object key, Object value) &#123; Object curValue = get(key); if (!Objects.equals(curValue, value) || (curValue == null &amp;&amp; !containsKey(key))) &#123; return false; &#125; remove(key); return true; &#125; //替换一个key的value，同时oldvalue要和之前的value相等 default boolean replace(K key, V oldValue, V newValue) &#123; Object curValue = get(key); if (!Objects.equals(curValue, oldValue) || (curValue == null &amp;&amp; !containsKey(key))) &#123; return false; &#125; put(key, newValue); return true; &#125; //也是替换一个key的value，但是不要求当前的value为何值 default V replace(K key, V value) &#123; V curValue; if (((curValue = get(key)) != null) || containsKey(key)) &#123; curValue = put(key, value); &#125; return curValue; &#125; //如果map中不存在key，则根据key通过mappingFuction得到一个value并放进去 default V computeIfAbsent(K key, Function&lt;? super K, ? extends V&gt; mappingFunction) &#123; Objects.requireNonNull(mappingFunction); V v; if ((v = get(key)) == null) &#123; V newValue; if ((newValue = mappingFunction.apply(key)) != null) &#123; put(key, newValue); return newValue; &#125; &#125; return v; &#125; //如果map中存在key，则根据key和oldvalue通过remappingFuction得到一个value并放入 default V computeIfPresent(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); V oldValue; if ((oldValue = get(key)) != null) &#123; V newValue = remappingFunction.apply(key, oldValue); if (newValue != null) &#123; put(key, newValue); return newValue; &#125; else &#123; remove(key); return null; &#125; &#125; else &#123; return null; &#125; &#125; //这个有点复杂，根据oldvalue(可能不存在)和key通过remappingFunction得到一个newValue //如果newValue不为null，则置换或者增加这个key，newValue //如果newValue为null，如果old也不为null，就把这个key移除出去 default V compute(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); V oldValue = get(key); V newValue = remappingFunction.apply(key, oldValue); if (newValue == null) &#123; // delete mapping if (oldValue != null || containsKey(key)) &#123; // something to remove remove(key); return null; &#125; else &#123; // nothing to do. Leave things as they were. return null; &#125; &#125; else &#123; // add or replace old mapping put(key, newValue); return newValue; &#125; &#125; //这个也有点复杂 //如果不存在这个键值对，就把key，value存进去 //如果存在，则根据oldvalue和value通过remappingFunction得到一个newvalue，如果newvalue为null，则把这个key移除，否则就存进去 default V merge(K key, V value, BiFunction&lt;? super V, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); Objects.requireNonNull(value); V oldValue = get(key); V newValue = (oldValue == null) ? value : remappingFunction.apply(oldValue, value); if(newValue == null) &#123; remove(key); &#125; else &#123; put(key, newValue); &#125; return newValue; &#125; Map的大概就是这些，我们主要了解了一些通用的default方法，通过lambda表达式可以很方便的做一些事。还有就是Map内部有一个Entry的接口，一个Entry，就是一个键值对。 AbstractMap123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229public abstract class AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; &#123; //key，value主要是存在这个entrySet中 public abstract Set&lt;Entry&lt;K,V&gt;&gt; entrySet(); //这里两个主要是为了得到所有的key集合或者value集合而设计的 transient Set&lt;K&gt; keySet; transient Collection&lt;V&gt; values; private static boolean eq(Object o1, Object o2) &#123; return o1 == null ? o2 == null : o1.equals(o2); &#125; //AbstractMap中有Entry的两个实现 //其中之一是SimpleEntry public static class SimpleEntry&lt;K,V&gt; implements Entry&lt;K,V&gt;, java.io.Serializable &#123; private final K key; private V value; //省略一些正常的构造函数和getter和setter //equals的实现是比较key和value是否相等 //其中eq函数是对Object的equals的封装，里面考虑了null的情况 public boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; return eq(key, e.getKey()) &amp;&amp; eq(value, e.getValue()); &#125; //hashCode()方法是对两个对象的hashCode求异或 public int hashCode() &#123; return (key == null ? 0 : key.hashCode()) ^ (value == null ? 0 : value.hashCode()); &#125; &#125; //第二个是SimpleImmutableEntry public static class SimpleImmutableEntry&lt;K,V&gt; implements Entry&lt;K,V&gt;, java.io.Serializable &#123; //可以看到key和value都是final的，所以自然没有setValue的方法，但是其实还是有的 private final K key; private final V value; //setValue方法抛出一个异常 public V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; //其他的和SimpleEntry的一样 &#125; //返回一个key的Set，这里使用了懒加载，只有在第一次调用的时候才会初始化keySet public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; if (ks == null) &#123; ks = new AbstractSet&lt;K&gt;() &#123; public Iterator&lt;K&gt; iterator() &#123; return new Iterator&lt;K&gt;() &#123; private Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); public boolean hasNext() &#123; return i.hasNext(); &#125; public K next() &#123; return i.next().getKey(); &#125; public void remove() &#123; i.remove(); &#125; &#125;; &#125; public int size() &#123; return AbstractMap.this.size(); &#125; public boolean isEmpty() &#123; return AbstractMap.this.isEmpty(); &#125; public void clear() &#123; AbstractMap.this.clear(); &#125; public boolean contains(Object k) &#123; return AbstractMap.this.containsKey(k); &#125; &#125;; //因为是懒加载，所以加载完之后还要再赋给keySet keySet = ks; &#125; return ks; &#125; //返回value的值集合，同样也是懒加载 public Collection&lt;V&gt; values() &#123; Collection&lt;V&gt; vals = values; if (vals == null) &#123; vals = new AbstractCollection&lt;V&gt;() &#123; public Iterator&lt;V&gt; iterator() &#123; return new Iterator&lt;V&gt;() &#123; private Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); public boolean hasNext() &#123; return i.hasNext(); &#125; public V next() &#123; return i.next().getValue(); &#125; public void remove() &#123; i.remove(); &#125; &#125;; &#125; public int size() &#123; return AbstractMap.this.size(); &#125; public boolean isEmpty() &#123; return AbstractMap.this.isEmpty(); &#125; public void clear() &#123; AbstractMap.this.clear(); &#125; public boolean contains(Object v) &#123; return AbstractMap.this.containsValue(v); &#125; &#125;; values = vals; &#125; return vals; &#125; //返回entry的大小 public int size() &#123; return entrySet().size(); &#125; //这个主要要注意是允许存放null值的，也是可以搜索null值的 public boolean containsValue(Object value) &#123; Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); if (value==null) &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (e.getValue()==null) return true; &#125; &#125; else &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (value.equals(e.getValue())) return true; &#125; &#125; return false; &#125; //同时key也是可以是null的 public boolean containsKey(Object key) &#123; Iterator&lt;Map.Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); if (key==null) &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (e.getKey()==null) return true; &#125; &#125; else &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (key.equals(e.getKey())) return true; &#125; &#125; return false; &#125; //get方法和上面的大同小异，，分为是null或者不是两种情况 public V get(Object key); //remove方法。。。嗯，，，好像也没什么好解释的，，看看就懂了 public V remove(Object key) &#123; Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); Entry&lt;K,V&gt; correctEntry = null; if (key==null) &#123; while (correctEntry==null &amp;&amp; i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (e.getKey()==null) correctEntry = e; &#125; &#125; else &#123; while (correctEntry==null &amp;&amp; i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (key.equals(e.getKey())) correctEntry = e; &#125; &#125; V oldValue = null; if (correctEntry !=null) &#123; oldValue = correctEntry.getValue(); i.remove(); &#125; return oldValue; &#125; //hashCode是把所有的Entry的hashCode加起来 public int hashCode() &#123; int h = 0; Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); while (i.hasNext()) h += i.next().hashCode(); return h; &#125; //这个toString写的很有意思，给人很有启发。。。主要是实现的比较优雅。。 public String toString() &#123; Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); if (! i.hasNext()) return \"&#123;&#125;\"; StringBuilder sb = new StringBuilder(); sb.append('&#123;'); for (;;) &#123; Entry&lt;K,V&gt; e = i.next(); K key = e.getKey(); V value = e.getValue(); sb.append(key == this ? \"(this Map)\" : key); sb.append('='); sb.append(value == this ? \"(this Map)\" : value); if (! i.hasNext()) return sb.append('&#125;').toString(); sb.append(',').append(' '); &#125; &#125; HashMap重头戏来了。。前面说过jdk1.7以前的HashMap的bin就是一个单链表，但是到了1.8的时候进行了改进。如果一个桶中的bin数量大于8并且当前的capacity大于64时，那么就会变成一颗红黑树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; //约定前面的数组的每一格称为桶 //约定桶后面存放的每一个数据称为bin //默认的桶的数量大小，写成这样不是直接写16是因为强调要是2的倍数 //为什么呢，往下看 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; //最大的桶的数量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //默认的负载，就是已有size／capacity，当达到这个值时，会进行resize，下面会提到 static final float DEFAULT_LOAD_FACTOR = 0.75f; //如果一个桶中的bin数量大于8，那么就变成红黑树，但是还有个条件见下 static final int TREEIFY_THRESHOLD = 8; //同上一个相反，当桶上的链表数小于这个值时树转链表 static final int UNTREEIFY_THRESHOLD = 6; //数化的还有一个条件是当哈希表中的容量大于这个值时，否则即使大于8，也不会树化。 static final int MIN_TREEIFY_CAPACITY = 64; //一些系统的参数 //系统的负载因子 final float loadFactor; //threshold表示当HashMap的size大于threshold时会执行resize操作。 //threshold=capacity*loadFactor int threshold; //这个table是开地址法的数组，相当于桶 transient Node&lt;K,V&gt;[] table; //放节点的Set transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; //目前有多少对map transient int size; //当前的HashMap被修改了多少次。 //还记得ConcurrentModificationException么 //这个在使用Iterator时，会判断当前的modCount和使用时是否相等 //如果不相等，那么就抛出异常。 transient int modCount; //先来看看构造函数 //第一个什么也没有，就是上面的值都是默认值 public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; &#125; //第二个可以指定初始的容量的大小，它调用了第三个构造函数。 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; //第三个构造函数主要对值进行了检查 //如果传进去的大小小于0，那就抛出一个异常，如果大于最大值，就设为最大值。 //但是loadFactor也进行了检查，不知道为什么，我好像没有看到一个方法可以对loadFactor进行修改。 public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; //还记得最开始说的容量最好是2的倍数吗？这里我们传一个数进去，它会找到最接近我们数的2的倍数作为容量。而不是就把我们的数作为容量了。具体函数见下 this.threshold = tableSizeFor(initialCapacity); &#125; //找到最接近cap的2的倍数的数 static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; //先来看看resize函数吧，这个是核心 final Node&lt;K,V&gt;[] resize() &#123; //先拿到当前数组桶的索引 Node&lt;K,V&gt;[] oldTab = table; //得到旧的table的长度，考虑到还没初始化，所以把null值作为0处理 int oldCap = (oldTab == null) ? 0 : oldTab.length; //旧的bin的阀值 int oldThr = threshold; int newCap, newThr = 0; //如果oldCap大于0，大概就是已经初始化过了。 if (oldCap &gt; 0) &#123; //如果已经达到最大值，就直接返回了，不可以在resize了。 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //不然看看把oldCap乘2，如果还没大于1 &lt;&lt; 30，就是默认的最大的桶数 //则新的cap的值就是就的值乘2 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; &#125; else if (oldThr &gt; 0) //不然新的桶的数量就是旧的阀值 newCap = oldThr; else &#123; //如果以上两个条件都不满足，那么newCap就是默认的值16，新的阀值就是12. newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //如果新的阀值为0 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; //直接new一个新的具有newCap大小的数组 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; //= =这里为什么不直接和上面的合成一句话呢。。。。 table = newTab; //下面就是把旧的table里的bin全部转移到新的table中 //其实很多文章说HashMap的put操作会造成死循环 //但是在1.8中进行了改进，所以下面这段代码在多线程环境中不会造成死循环 //旧的有个很好的文件进行解释,见下 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; //下面先来看看最常用的put函数，调用了putVal函数 public V put(K key, V value) &#123; //对于这儿的hash()函数，其实还是挺有意思的,作者觉得原来的容易产生 //碰撞，所以把前16位和后16为进行了异或。。。 return putVal(hash(key), key, value, false, true); &#125; static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //这里就是为什么capacity取2的整数次幂的原因了 //正常我们找index的时候都是hash % length，但是用到了除法 //如果2的整数次幂，那么hash % length = （length - 1) &amp; hash，用的是位运算 //对于计算机来说位运算的效率更高。 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果是树节点 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //如果大于8，进行进入treeifyBin函数，并不就是直接进行树化，因为还有一个条件。 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; //看看get方法，调用了getNode方法 public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; //如果这个桶里的节点是树节点，那么用找树节点的方法。见下 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; //下面看看树的操作。 //在HashMap中Entry的实现有两个 //第一个就是单链表的Node static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt;; //第二个是树的Node，TreeNode static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt;; //关于红黑树的原理，这里就不赘述了。。。完整说起来又是一遍长篇大论。 //将一个桶中的节点树化的函数 final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; //如果桶的大小小于64，进行的是resize而不是树化。 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125; &#125; //简单看一看TreeNode的定义。 static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; //还有指向父节点的指针。 TreeNode&lt;K,V&gt; parent; TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; boolean red; &#125; HashMap死循环 我们要知道死循环的原因是链表产生了回路。1.8之前产生回路的原因是换table时把原先的单链表每个节点放到了链表开头。就是说比如旧table[0]中 1 -&gt; 2 -&gt; 3在resize时，新的table[0]中变成 3 -&gt; 2 -&gt; 1 但是在1.8中不在这样，而是规规矩矩的还是1 -&gt; 2 -&gt; 3","link":"/2017/07/21/HashMap源码解析/"},{"title":"同步，阻塞，非阻塞","text":"同步，阻塞，非阻塞 先来谈谈这个。很多人一直搞不清，从概念上去解释的话，我也很难解释清楚。不过知乎上有个回答很nice。我引用过来。 同步异步，阻塞非阻塞的区别 正常的来说，一段数据流从网络到我们自己设定的数组中，需要经历以下几个阶段 数据到端口 内核把数据从端口拷贝到内核缓存区 用户调用read让内核把缓存区的数据拷贝到我们设定的数组中 同步和异步的区别在实际中就是用户和内核的交互上。同步的话就是我们主动的问内核数据来了吗，数据准备好了吗，如果内核高速我们数据已经到缓冲区了，我们还需要自己再去把缓冲区的数据拷过来。 异步就是内核把数据准备好了，拷贝到了用户空间后，之后主动调用我们设置的回掉函数。 阻塞和非阻塞在实际中是用户线程的IO和内核交互上。阻塞就是我们写或者读内核数据时必须等待全部写完或者必须有数据让我们读到。非阻塞是指IO操作被调用后立即返回给用户一个状态值，无需等到IO操作彻底完成。 下面结合代码来看看常用的IO模型是什么架构的。 阻塞IO 同步阻塞模型其实最常见也是最容易理解的。无论在Java还是c的网络编程中都是bind -&gt; listen -&gt; accept。 下面是一个简单的echo程序，把接收到的返回回去。 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;netdb.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;arpa/inet.h&gt;#define BACKLOG 1024#define BUFFERSIZE 8192int main(int argc, char *argv[])&#123; int server_fd, conn_fd; unsigned short port = 1080; struct sockaddr_in addr_s; struct sockaddr_in addr_c; //AF_INET ipv4 //SOCK_STREAM 以流的方式传递 server_fd = socket(AF_INET, SOCK_STREAM, 0); memset(&amp;addr_s, 0, sizeof(addr_c)); addr_s.sin_family = AF_INET; addr_s.sin_addr.s_addr = inet_addr(\"127.0.0.1\"); addr_s.sin_port = htons(port); int ret = bind(server_fd, (struct sockaddr*)&amp;addr_s, sizeof(addr_c)); char buffer[BUFFERSIZE]; int addr_clen = sizeof(addr_c); listen(server_fd, BACKLOG); while(1) &#123; conn_fd = accept(server_fd, (struct sockaddr*)&amp;addr_c, (socklen_t *) &amp;addr_clen); int size = read(conn_fd, buffer, 8192); write(conn_fd, buffer, size); close(conn_fd); &#125; return 0;&#125; 这里的while(1)中，如果没有连接，那么程序就一直等待accept，直到有一个连接产生。 等到有一个连接到了，然后去调用read和write函数去响应这个请求。 但是在处理这个read的时候如果对方一直没有输入，那么read就会一直等下去。那么read的行为就是阻塞。 同样的，对于write，如果发送缓冲区没有空间或者空间不足的话，write操作会直接阻塞住等待有足够的空间。 假设第一个连接停在了read或者write，那么下面的连接就无法进行处理了。 所以一般会进行fork调用去处理请求。 1234567if(fork() == 0) &#123; close(server_fd); int size = read(conn_fd, buffer, 8192); send(conn_fd, buffer, size, 0); close(conn_fd); return 0;&#125; 这样可以处理小批量的并发请求，但是大了就不行了。因为linux进程是需要资源，相对来说开销较大。就像Java中对每一个请求都开一个线程去处理一样。 非阻塞IO上面提到阻塞IO的问题在read和write函数。 在linux或者mac下有一个系统调用。 12#include &lt;fcntl.h&gt;int fcntl(int fildes, int cmd, ...); 使用这个我们可以改变文件描述符的性质。比如使它成为非阻塞的。 12345678910111213141516int set_no_blocking(int fd) &#123; int flags; //先得到文件描述符原有的属性 if ((flags = fcntl(fd, F_GETFL, 0)) == -1) &#123; log_err(\"fcntl error\\n\"); exit(1); &#125; //增加非阻塞属性 flags |= O_NONBLOCK; //设置非阻塞属性 if (fcntl(fd, F_SETFL, flags) == -1) &#123; log_err(\"fcntl error\\n\"); exit(1); &#125; return 1;&#125; 这样设置之后，read调用会立即返回。接收缓冲区中有数据时，与阻塞socket有数据的情况是一样的。 如果接收缓冲区中没有数据，则返回错误号为EWOULDBLOCK,表示该操作本来应该阻塞的，但是由于本socket为非阻塞的socket，因此立刻返回，遇到这样的情况，可以在下次接着去尝试读取。 如果返回值是其它负值，则表明读取错误。。 write也会立即返回。在发送缓冲区没有空间时会直接返回错误号EWOULDBLOCK,表示没有空间可写数据。如果错误号是别的值，则表明发送失败。如果发送缓冲区中有足够空间或者是不足以拷贝所有待发送数据的空间的话，则拷贝前面N个能够容纳的数据，返回实际拷贝的字节数。 IO复用 异步的最普遍的实现方式是回调。但是有回掉的并不就是异步的。 比如Libevent中事件回调，还有netty。 下面选自我的tinyhttp仓库，地址 12345678910111213141516171819202122232425262728293031323334353637383940414243//// Created by zhy on 2/20/17.//char* index_home = \"\";struct event_base *base;int main(int argc, char *argv[])&#123; argv[1] = \"127.0.0.1\"; argv[2] = \"4000\"; argv[3] = \"/Users/zhuyichen/fortest/tinydemo/v3.bootcss.com/\"; index_home = argv[3]; if (chdir(index_home) == -1) &#123; perror(\"index_home : \"); exit(1); &#125; int listener; listener = socket_bind_listen(argv[1], argv[2]); if (listener == -1) &#123; exit(1); &#125; set_no_blocking(listener); log_info(\"start listen in host %s port %s ...\\n\", argv[1], argv[2]); base = event_base_new(); struct event* ev_listen = event_new(base, listener, EV_READ | EV_PERSIST, on_accept, NULL); event_base_set(base, ev_listen); event_add(ev_listen, NULL); event_base_dispatch(base); event_base_free(base); &#125; void on_accept(int serverfd, short events, void *arg) &#123; int connfd = accept(serverfd, NULL, 0); set_no_blocking(connfd); &#125; 重点在以下代码中 123struct event* ev_listen = event_new(base, listener, EV_READ | EV_PERSIST, on_accept, NULL);event_base_set(base, ev_listen);event_add(ev_listen, NULL); 我们创建了一个ev_listen事件用来监听socket描述符上的READ事件,设置回调函数为on_accept。这样只要listener文件句柄上产生了新的连接，就是回调on_accept函数，在on_accept函数中进行accept。 这就是一个异步的例子。 很多人以为这就是异步，但是上面我们提到，异步的特点就是，操作系统会自己把数据从内核缓冲区拷贝到用户区，仔细看就会发现，libevent还是需要我们自己去进行read的，只是我们read的时候不会遇到缓冲区为空的情况，说到底，还是同步的读。 同步IO 我们再来谈谈同步的IO。其实同步异步非阻塞并不是相互独立的我们说同步就是内核的数据是否准备好需要我们自己去询问和如果准备好了还是需要我们自己去拷贝到用户空间。按照这个条件，其实前两种阻塞IO和非阻塞IO和IO复用都是同步IO。包括select poll epoll函数，都是同步的，因为我们还是需要自己区拷贝数据到用户空间。真正的异步IO这一层是系统帮我们做的。","link":"/2017/08/20/IO之同步，阻塞，非阻塞/"},{"title":"Java泛型","text":"Java泛型的实现提到Java的泛型一般人都会想到类型擦除(Type Erasure)机制。如果你没想到，请去补一补。。。。 创建泛型数组Java不允许创建泛型数组 1Hello&lt;String&gt;[] hello = new Hello&lt;String&gt;[12]; 这句话是编译不通过的。 答案有点让人失望的简单，因为数组必须知道里面的元素是什么。 但是这句话很误导人真的不允许创建我们想要的泛型数组么，那种可以编译时检查的泛型数组。答案是可以的，不过得通过一些技巧。 好，那么怎么创建呢。 1234@SuppressWarnings(\"unchecked\")public static void main(String[] args) &#123; List&lt;String&gt;[] lists = (List&lt;String&gt;[])new ArrayList[12];&#125; 这是其中之一的方法，可能有人觉得这样和 123public static void main(String[] args) &#123; List[] lists = new ArrayList[12];&#125; 没有区别。 划重点！！！！！第一种方法相对于第二种方法还是具有编译时类型检查的。 泛型类中的泛型数组怎么在范型类中创建泛型数组呢 其实第一来说没必要， 比如在ArrayList中维护的底层的数组是Object数组。 如果你硬是想创建，可以这样 1T[] arr = (T[])new Object[12]; 很多人或许觉得奇怪为什么，这不是向上转型么。 然后举出例子为什么 1List&lt;String&gt;[] list = (List&lt;String&gt;[])new Object[10]; 这种就不行。 答案还是Java泛型实现的核心类型擦除。在编译时 123T[] arr = (T[])new Object[12];//会变成//Object[] arr = (Object[])new Object[12]; 这样就是毫无问题。 协变，逆变协变和逆变 当A是B的子类时,如果有f(A)也是f(B)的子类,那么f叫做协变；当A是B的子类时,如果有f(B)是f(A)的子类,那么f叫做逆变； 数组是协变的。 12345678910111213public class Genetic &#123; public static void main(String[] args) &#123; Fruit[] fruits; Apple[] apples = new Apple[10]; fruits = apples; &#125;&#125;class Apple extends Fruit &#123; &#125;class Fruit &#123; &#125; 这段代码能编译通过。 但是这一段就不行了 12345678910111213public class Genetic &#123; public static void main(String[] args) &#123; List&lt;Fruit&gt; list; List&lt;Apple&gt; list1 = new ArrayList&lt;&gt;(); list = list1; &#125;&#125;class Apple extends Fruit &#123; &#125;class Fruit &#123; &#125; 就是说List和List没有关系。 java通过通配符解决这个问题，还是上面的代码 12345678910111213public class Genetic &#123; public static void main(String[] args) &#123; List&lt;? extends Fruit&gt; list; List&lt;Apple&gt; list1 = new ArrayList&lt;&gt;(); list = list1; &#125;&#125;class Apple extends Fruit &#123; &#125;class Fruit &#123; &#125; extends解决了协变的问题，逆变通过super解决。 但是，没有那么简单。 12345678910111213public class Genetic &#123; public static void main(String[] args) &#123; List&lt;? extends Fruit&gt; list = new ArrayList&lt;Apple&gt;(); //这句话并不能通过编译 //list.add(new Apple()); &#125;&#125;class Apple extends Fruit &#123; &#125;class Fruit &#123; &#125; &lt;? extends Fruit&gt;不是表示里面的元素只要是Fruit的子类就行，而是加入的元素必须可以向上转型为Fruit的任意一个子类。那么大概只有null可以add了。 同样的&lt;? super Fruit&gt;表示里面元素可以转型为Fruid的任意一个父类。 要想可以编译通过，我们可以使用super 12345678910111213public class Genetic &#123; public static void main(String[] args) &#123; List&lt;? super Fruit&gt; list = new ArrayList&lt;&gt;(); list.add(new Apple()); //ok &#125;&#125;class Apple extends Fruit &#123; &#125;class Fruit &#123; &#125; 那到底应该什么时候用super和extends呢。Effective Java中总结了一个原则 producer-extends, consumer-super 看看ArrayList的源码就知道了 12345678public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0;&#125; 12345678910111213public void forEach(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); final int expectedModCount = modCount; @SuppressWarnings(\"unchecked\") final E[] elementData = (E[]) this.elementData; final int size = this.size; for (int i=0; modCount == expectedModCount &amp;&amp; i &lt; size; i++) &#123; action.accept(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;","link":"/2017/08/09/Java泛型数组/"},{"title":"LinkedList源码分析","text":"前言123public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable 和ArrayList相比，LinkedList多了AbstractSequentialList类和Deque接口。 其中Deque又是继承自Queue，所以LinkedList可以当做队列或者双端队列。 其中AbstractSequentialList的作用可以当做是RandomAccess的对照来看。 关于RandomAccess，可以看看ArrayList源码。 底层结构LinkedList每个节点都是一个Node类，Node中有next和prev。可以看到实现的是双端队列。1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; 基本的成员变量，保存了链表的头部和尾部123transient Node&lt;E&gt; first;transient Node&lt;E&gt; last;transient int size = 0; 基本操作get方法123456789101112131415161718public E get(int index) &#123; checkElementIndex(index); return node(index).item; &#125;Node&lt;E&gt; node(int index) &#123; if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125; &#125; get方法调用了node方法，node方法是找到index位的元素。这个方法还是很有意思的，他先判断index是在前半段还是后半段，前半段就从first节点开始找，后半段就从last节点开始找。 remove方法123456789101112131415161718public boolean remove(Object o) &#123; if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); return true; &#125; &#125; &#125; return false; &#125; 因为LinkedList支持存null，所以在remove的时候要分两种情况讨论。 迭代IteratorIterator方法是在AbstractSequentialList中123public Iterator&lt;E&gt; iterator() &#123; return listIterator(); &#125; LinkedList中重写了listIterator方法。1234public ListIterator&lt;E&gt; listIterator(int index) &#123; checkPositionIndex(index); return new ListItr(index); &#125; ListLtr是LinkedList中的一个类1private class ListItr implements ListIterator&lt;E&gt; ListIterator1234public ListIterator&lt;E&gt; listIterator(int index) &#123; checkPositionIndex(index); return new ListItr(index); &#125; 可以看到，其实和Iterator方法的实现是一样的。","link":"/2017/12/10/LinkedList源码分析/"},{"title":"LongAdder解析","text":"12345678public class LongAdder extends Striped64 implements Serializable &#123; &#125; abstract class Striped64 extends Number &#123; transient volatile Cell[] cells; transient volatile long base;&#125; 主要思路就先cas那个base变量，如果race condition不严重的话，那么就成功返回。 但是如果race condition比较严重，那么就新建很多的cell，在cell里增加value，这样就降低了很多的并发阻塞。 最后汇总的时候，除了base，还需要把每个cell中的value也加上去。 12345678910public void add(long x) &#123; Cell[] as; long b, v; int m; Cell a; if ((as = cells) != null || !casBase(b = base, b + x)) &#123; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[getProbe() &amp; m]) == null || !(uncontended = a.cas(v = a.value, v + x))) longAccumulate(x, null, uncontended); &#125;&#125; 第一个if，表示如果cells还是为空，并且cas增加到base中失败，那么就进行第二个if中的操作。第二个if中，如果as为空，或者as未完全初始化 getProbe() &amp; m相当于一次hash找到对应的cell，如果cell为空，或者cell不为空，但是cell自身的cas加值还是失败，那么最终会调用longAccumulate方法。 longAccumulate方法在Striped64类中，其中我们由add中传入的参数是 要增加的x null false 下面的方法就是对引入的cell就行操作，包括增加，扩容 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192transient volatile int cellsBusy; final void longAccumulate(long x, LongBinaryOperator fn, boolean wasUncontended) &#123; int h; //得到与线程相关的hashCode。 if ((h = getProbe()) == 0) &#123; ThreadLocalRandom.current(); // force initialization h = getProbe(); wasUncontended = true; &#125; boolean collide = false; // True if last slot nonempty for (;;) &#123; Cell[] as; Cell a; int n; long v; //如果cell已经不为null，完成初始化。 if ((as = cells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; //找到与当前线程有关的cell，和HashMap有点像 //如果当前cell为空，那么就进行初始化 if ((a = as[(n - 1) &amp; h]) == null) &#123; //cellsBusy相当于一个锁，用volatile修饰，== 0时表示可以对cells进行操作。 if (cellsBusy == 0) &#123; // Try to attach new Cell Cell r = new Cell(x); // Optimistically create if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; boolean created = false; try &#123; // Recheck under lock Cell[] rs; int m, j; if ((rs = cells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) &#123; rs[j] = r; created = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (created) break; continue; // Slot is now non-empty &#125; &#125; collide = false; &#125; //检查是否发生了一次cas失败，如果是那就标为true，进行下一次循环 else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash //调用cell本身的cas else if (a.cas(v = a.value, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; else if (n &gt;= NCPU || cells != as) collide = false; // At max size or stale else if (!collide) collide = true; //对cells进行扩充 else if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; try &#123; if (cells == as) &#123; // Expand table unless stale Cell[] rs = new Cell[n &lt;&lt; 1]; for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; cells = rs; &#125; &#125; finally &#123; cellsBusy = 0; &#125; collide = false; continue; // Retry with expanded table &#125; h = advanceProbe(h); &#125; //如果cell还是为空，那么就进行自旋，如果得到锁，那么就进行初始化 //初始化为2个的cell，finally释放自旋锁 else if (cellsBusy == 0 &amp;&amp; cells == as &amp;&amp; casCellsBusy()) &#123; boolean init = false; try &#123; // Initialize table if (cells == as) &#123; Cell[] rs = new Cell[2]; rs[h &amp; 1] = new Cell(x); cells = rs; init = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (init) break; &#125; //最后还是尝试加在base上，如果失败，那就再来一次for循环。 else if (casBase(v = base, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // Fall back on using base &#125;&#125; 这一段代码的for循环中可以分为三部分。 第一个部分，如果cells已经完成初始化。 如果cell还是空，那么就自旋，把新的cell插到cells中。 如果已经发生了一次cas失败，那么就标为true，以便下一次cas 如果cell不为空，那么调用cell的cas 如果各种方法都试了，那么没办法了，尝试扩充cells 第二个部分，因为cell还未完成初始化，那么就拿到自旋锁进行初始化。 第三个部分，如果已经被别的线程在进行初始化，那么就再次尝试调用加在base上。如果失败，那么就再来一次for循环。 可以看到每次尝试失败，那么就调用代价更高的方法再进行尝试。所有方法用尽了，那没办法，再重新试一次吧。 可以看到LongAdder和AtomicLong比起来，都是cas的运用。AtomicLong如果cas失败，那么就进行while循环进行cas。但是LongAdder把cas的压力分散到很多个cell中，最后加的时候把每个cell中的value都加上去。高并发效率上，肯定是LongAdder更高一点，但是LongAdder相应的也更费空间。 另外，在ConcurrentHashMap中就是运用了LongAdder的思想。 参考：https://coolshell.cn/articles/11454.html","link":"/2018/02/23/LongAdder解析/"},{"title":"PathVariable及路由设计","text":"项目源码@PathVariable在web后端框架中，路由设计是很重要的一步。各个框架对接口设计的支持大多差不多，其中必然有 12@GET(&quot;/users/&#123;id&#125;/&quot;) @GET(&quot;/users/:id/&quot;) 这样的标准。 其中{id}和:id表示是一个不确定的值，这个随着rustful的兴起也变得常用起来比如我想要查看id = 1的用户的信息，那么我的请求便是/users/1如果查看id = 321的用户信息，请求的便是/users/321。 那么这个id便是一个不确定量。 于是对于这个方法而言这个id值便是参数，在我们调用时需要传进去。 在Spring MVC中，完整的方法是这样 1234@GetMapping(\"/employer/&#123;phoneNumber&#125;\") public ResponseEntity&lt;?&gt; findByPhoneNumber(@PathVariable String phoneNumber) &#123; &#125; 所以这就要求我们在设计框架时还需要利用反射把请求的uri的特定路径值作为参数传进方法中。 route设计正常我们进行反射调用时，像这样 1method().invoke(object(), paramters()); 首先route中需要有这个对应的Method，其次对应的对象也应该在其中，然后是参数。 于是我设计为 123456789101112class Route &#123; //对应的uri private String path; //method所在的对象 private Object object; //对应的路由method private Method method; //对应的方法，比如GET，POST等 private HttpMethod httpMethod; //对应的参数 private Object[] paramters;&#125; 路由匹配那么问题来了，对于不同的uri，我们应该怎么匹配路由呢。 我想到的是用正则设计一个这样的Map 1Map&lt;Pattern, Route&gt; routeMap = new HashMap&lt;&gt;(); 对于/users/{id}/这样的字符串，我们需要产生一个pattern出来。 123456789101112131415161718192021222324//根据参数进行正则替换public static Pattern pathCompiler(String path, Method method) &#123; Parameter[] parameters = method.getParameters(); for (Parameter parameter : parameters) &#123; if (parameter.getAnnotations() == null) &#123; continue; &#125; Annotation annotation = parameter.getAnnotations()[0]; if (annotation instanceof PathVariable) &#123; //如果是字符串 if (parameter.getType() == String.class) &#123; path = path.replace(\"&#123;\" + parameter.getName()+\"&#125;\",\"[0-9\\\\d\\\\D]*\"); &#125; //如果是数字 else if (parameter.getType() == Integer.class || parameter.getType() == Long.class) &#123; path = path.replace(\"&#123;\" + parameter.getName()+\"&#125;\",\"[0-9]*\"); &#125; &#125; &#125; return Pattern.compile(path);&#125; 键为url的Pattern，那么在匹配时只要遍历一下 1234567891011121314//保存着所有的路由Map&lt;Pattern, Route&gt; routeMap = new HashMap&lt;&gt;(); public static Route findRoute(String path, HttpMethod method) &#123; for (Pattern pattern : routeMap.keySet()) &#123; if (pattern.matcher(path).matches()) &#123; if (routeMap.get(pattern).getHttpMethod().equals(method)) &#123; /* 进行参数的赋值 *／ return routeMap.get(pattern); &#125; &#125; &#125; return null;&#125;","link":"/2017/07/21/PathVariable及route设计/"},{"title":"ThreadLocal源码分析","text":"前言ThreadLocal类的作用就是对于每个线程，保存一份类的实例。 简单的说实现的话，就是存一个map，key就是当前的线程，value是该变量。但是实际上的实现还是和我最初想的不一样，这个还要更复杂点。 底层结构ThreadLocalThreadLocal中有三个变量，其中只有一个是成员变量。12345678910111213141516private final int threadLocalHashCode = nextHashCode(); //这个可以看做是ThreadLocal的hashcode()，但是是通过nextHashCode得到的。 private final int threadLocalHashCode = nextHashCode(); private static AtomicInteger nextHashCode = new AtomicInteger(); //这个是每次增加的数，为啥要设为这个奇怪的数呢，我百度了下 //0x61c88647可以使 hashcode 均匀的分布在大小为 2 的 N 次方的数组里 //叫做Fibonacci Hash，这是我们下面进行Entry分配的基础。 private static final int HASH_INCREMENT = 0x61c88647; private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT); &#125; ThreadLocalMap每个Entry的key存的是ThreadLocal而并不是线程。 和HashMap的分离链表法不同，ThreadLocalMap中处理冲突的方法是开放定址法。不会产生链表，而是往下找。这就要求我们散列分的比较均匀。这也是采用Fibonacci的原因。12345678910111213141516171819202122232425262728293031323334//这个Map的Entry继承了WeakReferenece，也就是弱引用。//如果这个对象只被弱引用引用，那么它只能活到下一次gc前。//这个把key作为一个弱引用，就是为了在我们ThreadLocal对象在不被使用后//能被GC自动清除，避免内存泄漏//当我们发现key == null的时候，就可以进行清理value的工作。static class ThreadLocalMap &#123; static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; //初始的默认大小，参照HashMap的话，这里也必须是2的整数次方。 private static final int INITIAL_CAPACITY = 16; //底层数组 private Entry[] table; private int size = 0; //下一次要扩展的容量大小。 private int threshold; //构造函数，key是ThreadLocal ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; //这个可以看做是Hash函数。因为是通过的Fibonacci Hashing实现的。 int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125; //其他的方法我们下面结合ThreadLocal方法看。&#125; 其中这个map的实例并不是放在ThreadLocal中，而是在Thread中。 123public class Thread implements Runnable &#123; ThreadLocal.ThreadLocalMap threadLocals = null;&#125; 虽然是在Thread类中，但是还是在ThreadLocal中进行操作的。Thread类很少对这个类进行操作。 SuppliedThreadLocal这个类的定义在ThreadLocal中，提供了ThreadLocal的初始值。正常的我们ThreadLocal如果没有进行set就get的话会得到一个null。 12345678910111213static final class SuppliedThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; &#123; private final Supplier&lt;? extends T&gt; supplier; SuppliedThreadLocal(Supplier&lt;? extends T&gt; supplier) &#123; this.supplier = Objects.requireNonNull(supplier); &#125; @Override protected T initialValue() &#123; return supplier.get(); &#125; &#125; 这个类重写了initialValue方法，返回我们指定的初始值。如果我们想得到这个类的实例，ThreadLocal提供了一个静态方法123public static &lt;S&gt; ThreadLocal&lt;S&gt; withInitial(Supplier&lt;? extends S&gt; supplier) &#123; return new SuppliedThreadLocal&lt;&gt;(supplier); &#125; 这个是在1.8中新增的。 基本操作123void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125; 方法定义在ThreadLocal中，给线程t创建一个ThreadLocalMap，key为自己。 123ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125; 返回线程t的ThreadLocalMap 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; &#125; &#125; return setInitialValue(); &#125;//ThreadLocalMap中的getEntry方法//首先进行Hash找到entry，如果发现key和我们的不同，那就是发生了两种情况//第一种是已经被GC清除了，或者产生了冲突，这时候我们调用getEntryAfterMiss方法。private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e); &#125;private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; while (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) return e; //如果是被GC收集掉了，那么调用expungeStaleEntry方法把后面的重新Hash if (k == null) expungeStaleEntry(i); else //不然就是继续找下一个桶。 i = nextIndex(i, len); e = tab[i]; &#125; return null; &#125;private int expungeStaleEntry(int staleSlot) &#123; Entry[] tab = table; int len = tab.length; //做清除工作。 tab[staleSlot].value = null; tab[staleSlot] = null; size--; //将后面的Entry重新Hash Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; return i; &#125; 总结 key为弱引用，避免内存泄漏 采用开地址法，运用Fibonacci Hash是hashcode分配均匀。 参考https://www.cnblogs.com/zhangjk1993/archive/2017/03/29/6641745.html","link":"/2017/12/21/ThreadLocal源码分析/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2017/07/07/hello-world/"},{"title":"fat-jar配置","text":"前言maven打包的时候，默认只是把你的源文件打包起来如果你的项目中还依赖了其他的项目，那是不打包进去的。 这样我们在启动的时候，配置起来可能就会很麻烦。 于是fat-jar的方式诞生了。 比如最熟悉的spring-boot项目，我们在package时候，在target目录的jar文件我们可以直接java -jar 运行。不需要手动管理依赖。 配置有好几个maven插件支持fat-jar的方式。介绍一下spring-boot的插件。 12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;&#123;version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;mainClass&gt;&#123;MainClass&#125;&lt;/mainClass&gt; &lt;layout&gt;JAR&lt;/layout&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 同时，需要进行java-version的一些配置 12345&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;/properties&gt; 运行运行的话，可以直接运行或者打包好后运行mvn spring-boot:run mvn package在target目录会生成一个大的jar文件java -jar XXXX.jar就可以启动了","link":"/2018/02/10/fat-jar/"},{"title":"JsessionId的生成","text":"前言因为http是无状态的，所以产生了cookie，session机制来构造一个会话。 一般流程是这样： 客户端发送请求 服务端接收请求，并且设置Set-Cookie头，分配一个sessionId 客户端存在cookie，下次发送时带上sessionid 服务端解析sessionid，找到对应的session信息 所以我们可以知道，核心就在这个sessionid身上。 那么这个sessionid需要哪些特质呢 唯一性:这个是肯定的，万一前一个人请求，发送sessionid，后面紧接着又来一个人，发送一个sessionid，两个人的id重合了，那就出问题了。 不容易猜到性:这个其实是阻拦一些攻击的。java中自带了UUID 1UUID.randomUUID().toString().replaceAll(\"-\", \"\"); 这样做其实可以满足唯一性，但是使用UUID的一个问题是，UUID其实并不是那么不好猜。因为UUID的使用领域就不是为了安全领域的。参见StackOverFlow https://stackoverflow.com/questions/5244455/best-practices-for-sessionid-authentication-token-generation Tomcat的实现由于本人算法渣，自己想不到什么好办法实现，所以就去看了Tomcat怎么实现。 SecureRandom123456//SecureRandom是在java.security包中//主要是用来产生随机数//这个类主要依赖sun.security.provider.SecureRandom()我是没看明白。。。//下面的sessionid的实现主要也是利用这个类实现的public class SecureRandom extends java.util.Random &#123;&#125; SessionIdGenerator123456789101112131415public interface SessionIdGenerator &#123; //这个参数和集群有关，下面会看到 public String getJvmRoute(); public void setJvmRoute(String jvmRoute); //sessionid的长度 public int getSessionIdLength(); //设置sessionid的长度 public void setSessionIdLength(int sessionIdLength); //这是最核心的方法，下面会看到他的实现 public String generateSessionId(); public String generateSessionId(String route);&#125; SessionIdGeneratorBase123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public abstract class SessionIdGeneratorBase extends LifecycleBase implements SessionIdGenerator &#123; //这里保存了很多的SecureRandom实例，因为单个的SecureRandom使用了同步 //这样如果队列为空，就创建一个SecureRandom实例，这样会快点 private final Queue&lt;SecureRandom&gt; randoms = new ConcurrentLinkedQueue&lt;&gt;(); private String secureRandomClass = null; private String secureRandomAlgorithm = \"SHA1PRNG\"; private String secureRandomProvider = null; //这个参数很有意思，如果是单机上的，就是\"\"，如果是在tomcat集群中，那么这个就是集群的编号 private String jvmRoute = \"\"; //默认长度是16 private int sessionIdLength = 16; //产生一个sessionid //如果是单机的话，就是调用了 //generateSessionId(\"\") public String generateSessionId() &#123; return generateSessionId(jvmRoute); &#125; //利用queue中的SecureRandom产生随机数 //这里可以看到为什么要用一个队列存SecureRandom实例了。 protected void getRandomBytes(byte bytes[]) &#123; SecureRandom random = randoms.poll(); if (random == null) &#123; random = createSecureRandom(); &#125; random.nextBytes(bytes); randoms.add(random); &#125; //这里产生一个SecureRandom实例，一般来说会从找是否设置了 //自定义的SecureRandom，如果没有，就创建一个系统的 private SecureRandom createSecureRandom() &#123; SecureRandom result = null; long t1 = System.currentTimeMillis(); if (secureRandomClass != null) &#123; try &#123; // Construct and seed a new random number generator Class&lt;?&gt; clazz = Class.forName(secureRandomClass); result = (SecureRandom) clazz.getConstructor().newInstance(); &#125; catch (Exception e) &#123; log.error(sm.getString(\"sessionIdGeneratorBase.random\", secureRandomClass), e); &#125; &#125; boolean error = false; if (result == null) &#123; ... &#125; if (result == null &amp;&amp; error) &#123; .... &#125; //用系统的 if (result == null) &#123; // Nothing works - use platform default result = new SecureRandom(); &#125; // Force seeding to take place result.nextInt(); long t2 = System.currentTimeMillis(); if ((t2 - t1) &gt; 100) &#123; log.warn(sm.getString(\"sessionIdGeneratorBase.createRandom\", result.getAlgorithm(), Long.valueOf(t2 - t1))); &#125; return result; &#125;&#125; StandardSessionIdGenerator1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//这个类只重写了一个generateSessionId方法public class StandardSessionIdGenerator extends SessionIdGeneratorBase &#123; public String generateSessionId(String route) &#123; byte random[] = new byte[16]; int sessionIdLength = getSessionIdLength(); StringBuilder buffer = new StringBuilder(2 * sessionIdLength + 20); int resultLenBytes = 0; while (resultLenBytes &lt; sessionIdLength) &#123; //这个方法就是利用SecureRandom得到随机数 getRandomBytes(random); for (int j = 0; j &lt; random.length &amp;&amp; resultLenBytes &lt; sessionIdLength; j++) &#123; //对产生的随机数进行一些处理 byte b1 = (byte) ((random[j] &amp; 0xf0) &gt;&gt; 4); byte b2 = (byte) (random[j] &amp; 0x0f); if (b1 &lt; 10) buffer.append((char) ('0' + b1)); else buffer.append((char) ('A' + (b1 - 10))); if (b2 &lt; 10) buffer.append((char) ('0' + b2)); else buffer.append((char) ('A' + (b2 - 10))); resultLenBytes++; &#125; &#125; //加上集群中的本机编号 if (route != null &amp;&amp; route.length() &gt; 0) &#123; buffer.append('.').append(route); &#125; else &#123; String jvmRoute = getJvmRoute(); if (jvmRoute != null &amp;&amp; jvmRoute.length() &gt; 0) &#123; buffer.append('.').append(jvmRoute); &#125; &#125; return buffer.toString(); &#125;&#125; 总结总的来说 Tomcat的sessionid的生成主要是随机数，依赖的类是java.security.SecureRandom 为了避免多线程竞争的问题，引入了一个queue，当SecureRandom不够用就生成一个。 同时在集群中加入了本机的编号","link":"/2017/11/18/jsessionid的生成/"},{"title":"libevent简单使用","text":"最近想要自己写一个简单的http server，因为之前的那个是epoll的，只能运行在linux平台上。于是我便找到了平台通用的libevent。 libevent是事件驱动的，支持异步的网络框架。 十分值得学习。 由于基于reactor模型，所以事件抽象为event，event loop是reactor，event handler是各种回调函数。 struct event要创建event一般使用event_new函数。 12struct event *event_new(struct event_base *base, evutil_socket_t fd, short what, event_callback_fn cb, void *arg); base参数是我们要把这个事件绑定的event loop，fd就是socket fd，只不过为了统一windows，重新定义了一下what是触发事件，选项是下面的或 123456#define EV_TIMEOUT 0x01#define EV_READ 0x02#define EV_WRITE 0x04#define EV_SIGNAL 0x08#define EV_PERSIST 0x10#define EV_ET 0x20 cb就是触发事件的回调函数，arg是我们自定义传进去的参数。 而回调函数的格式是一样的。 比如。 1void on_write(int conn_fd, short event, void *arg) 第一个conn_fd就是我们event_new 中的fd， 第二个便是what，第三个是arg。 struct event_base创建event_base使用。 1struct event_base* event_base_new() 把一个event注册到event_base中 1234struct event *base = event_base_new();struct event* ev_listen = event_new(base, listen_fd, EV_READ | EV_PERSIST, on_accept, NULL);event_base_set(base, ev_listen);event_add(ev_listen, NULL); 开始执行loop 1int event_base_dispatch(struct event_base *base); 还有类似free函数 12event_base_free(struct event base*);event_free(struct event*); 一个简答的example打开一个socket，每有一次连接进来便接受并关闭。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354int socket_bind_listen(char *addr, char *port) &#123; int listen_fd; unsigned short Port; struct sockaddr_in addr_s; //AF_INET ipv4 //SOCK_STREAM 以流的方式传递 listen_fd = socket(AF_INET, SOCK_STREAM, 0); if (listen_fd == -1) &#123; log_err(\"%s\", strerror(errno)); exit(1); &#125; //get port Port = (unsigned short) atoi(port); memset(&amp;addr_s, 0, sizeof(addr_s)); addr_s.sin_family = AF_INET; addr_s.sin_addr.s_addr = inet_addr(addr); addr_s.sin_port = htons(Port); int ret = bind(listen_fd, (struct sockaddr *) &amp;addr_s, sizeof(addr_s)); if (ret == -1) &#123; perror(\"bind:\"); close(listen_fd); exit(1); &#125; if (listen(listen_fd, BACKLOG) == -1) &#123; perror(\"listen:\") close(listen_fd); exit(1); &#125; return listen_fd;&#125; void on_accept(int listen_fd, short events, void *arg) &#123; int conn_fd = accept(listen_fd, NULL, 0); if (conn_fd &lt; 0) &#123; return; &#125; printf(\"get connect\\n\"); close(conn_fd);&#125; int main(int argc, char *argv[])&#123; int listen_fd = socket_bind_listen(\"127.0.0.1\", \"8080\"); struct event_base *base = event_base_new(); struct event* ev_listen = event_new(base, listen_fd, EV_READ | EV_PERSIST, on_accept, NULL); event_base_set(base, ev_listen); event_add(ev_listen, NULL); event_base_dispatch(base); event_base_free(base); event_free(ev_listen); &#125;","link":"/2017/08/02/libevent简单使用/"},{"title":"log4j和slf4j配置","text":"前言之前自己写的时候哪管什么log不log的，遇到啥都是直接sout。后来在写自己的框架的时候，发现不行啊，我要做一个log出来，像spring启动那样。来了美团，这里的log更加的严谨了，除了要运行的时候打印出log，还要把log分类分时间打到本地的文件里， 当然这还不够，美团有个cat框架，还需要我们把log通过网络，打到专门管理log的服务器上去。 大的系统log是很有必要的，这样出了问了log能给我们很大的启示。 Log框架Java的Log框架是比较乱的，之前一直没搞懂，不知道依赖关系。 主要的有 apache 的log4j 升级版的log4j2 apache的common logging java.util.Logging slf4j LogBack 不自己去了解一下其实是很乱的，光apache的log就有好几套。 不过主流的还是log4j和LogBack，但是为了避免框架的Log和用户的Log不一样。总不能一个系统里就有几套不同的Log吧。于是slf4j出现了，这个框架，不提供实现，只提供接口。幸运的是Log4j和LogBack都实现了这个框架的接口。所以现在大多数还是直接调用slf4j的接口。实现的话，我们选一个去import一下就行。 依赖1234567891011&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.10.0&lt;/version&gt;&lt;/dependency&gt; 以log4j为例，首先我们得引入slf4j的规定的接口包，然后再引入log4j的实现包就行了。 再加上log的配置文件。（见下） 在代码中就直接写123456public class Main &#123; private static final Logger log = LoggerFactory.getLogger(Main.class); public static void main(String[] args) &#123; log.info(\"dddddddd\"); &#125;&#125; 就可以了。 配置文件log4j的配置文件其实也不难。在resources目录下建立log4j2.xml文件。1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;Configuration&gt; &lt;Properties&gt; &lt;Property name=\"log-path\"&gt;/Users/zhuyichen/log/testLogs.txt&lt;/Property&gt; &lt;/Properties&gt; &lt;Appenders&gt; &lt;Console name=\"Console\" target=\"SYSTEM_OUT\"&gt; &lt;PatternLayout pattern=\"%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n\"/&gt; &lt;/Console&gt; &lt;File name=\"file\" fileName=\"$&#123;log-path&#125;\" append=\"false\"&gt; &lt;PatternLayout pattern=\"%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n\"/&gt; &lt;/File&gt; &lt;RollingFile name=\"rollingFile\" fileName=\"$&#123;log-path&#125;\" filePattern=\"$&#123;sys:user.home&#125;/log/$$&#123;date:yyyy-MM&#125;/info-%d&#123;yyyy-MM-dd&#125;-%i.log\" append=\"true\"&gt; &lt;ThresholdFilter level=\"info\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; &lt;PatternLayout pattern=\"[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n\"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size=\"100MB\"/&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max=\"20\"/&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Logger name=\"Main\" level=\"error, info\" additivity=\"false\"&gt; &lt;AppenderRef ref=\"Console\"/&gt; &lt;/Logger&gt; &lt;Root level=\"all\"&gt; &lt;AppenderRef ref=\"Console\"/&gt; &lt;AppenderRef ref=\"file\"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; Properties首先Properties的条目是随便写的，只是方便我们下面引用的时候方便管理而已。比如我们把我的log目录文件路径放进去了。下面就可以用${log-path}来引用 Appenders下面就是Appenders，这里相当于我们设定几个log模式。 Console是console直接打印的模式。PatternLayout是打印出来的样式。 File就是打印到文件的模式。 name 名字，由下面的Logger引用用 fileName Log文件的路径 append 每次启动，log是加到文件里，还是清空文件。 RollingFile就是轮转的Log，可以指定策略创建新的，删除旧的。叫滚动日志 name Appenders的名字 fileName文件的名字 filePattern就是指定创建新的log文件的格式 Policies 指定滚动日志的策略 TimeBasedTriggeringPolicy 基于时间的滚动策略 SizeBasedTriggeringPolicy 基于指定文件大小的滚动策略，size属性用来定义每个日志文件的大小 DefaultRolloverStrategy 用来指定同一个文件夹下最多有几个日志文件时开始删除最旧的 LoggersLoggers才是真正控制我们Log的地方。 一般有两种节点，一个是Logger，一个是Root。 level:日志输出级别，共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF。 按这个级别排序呢，就是说如果是all，那么大于等于它的级别的都会输出。 Logger一般单独指认某个类或者某个的输出日志。 name 指某个类所在全程或者某个包的全程 level 输出的日记级别 additivity 如果设为false，那么就不会在下面的Root节点再输出了。 Root是个特殊的条目，就是剩下来的所有相比较Logger配置只有一个level 在每个配置中具体日志打到哪儿，需要引用我们上面定义的Appender了。你需要这个Log打印到哪儿，就在里面增加一个AppenderRef条目。 比如我的Root节点需要全部既打印到终端，又打印到文件，我就把两个都引用进去。 参考https://www.cnblogs.com/hafiz/p/6170702.html官方文档","link":"/2018/02/07/log4j和slf4j配置/"},{"title":"pigeon中的核心类和作用","text":"ServiceFactory这个类的作用就是服务的注册发布和get的最外层接口 内部有两个主要的类 12private static ServiceProxy serviceProxy = ServiceProxyLoader.getServiceProxy();private static PublishPolicy publishPolicy = PublishPolicyLoader.getPublishPolicy(); ServiceProxy用来获取service的，就是包一层proxy吧。 PublishPolicy简单说就是发布service的，但是内部还是借助于ServicePublisher进行操作的。 RegistryManager这个类的作用其实是为ServicePublisher的addService到zookeeper中和unpublish指定的service提供API支持的。 更通俗点就是为invoker和provider提供了zookeeper的交流的。 但是内部还是使用的registry类进行和zookeeper调用的。 1234567891011public interface Registry &#123; //删除了好多，可以自己去看源码 // for invoker boolean isSupportNewProtocol(String serviceAddress, String serviceName) throws RegistryException; // for provider void setSupportNewProtocol(String serviceAddress, String serviceName, boolean support) throws RegistryException; // for invoker/provider RegistryConfig getRegistryConfig(String ip) throws RegistryException;&#125; InvokerBootStrap这个类看名字也知道是为了invoker提供方便的。 看调用也是AbstractServiceProxy这个类调用的最多 主要有两个方法，startup和shutdown，也没什么重要的成员变量。 12345678910111213141516171819public static void startup() &#123; if (!isStartup) &#123; synchronized (InvokerBootStrap.class) &#123; if (!isStartup) &#123; ServiceInvocationRepository.getInstance().init(); InvokerProcessHandlerFactory.init(); SerializerFactory.init(); LoadBalanceManager.init(); RegionPolicyManager.INSTANCE.init(); Monitor monitor = MonitorLoader.getMonitor(); if (monitor != null) &#123; monitor.init(); &#125; isStartup = true; logger.warn(\"pigeon client[version:\" + VersionUtils.VERSION + \"] has been started\"); &#125; &#125; &#125;&#125; 其中ServiceInvocationRepository的init方法主要是启动了一个InvocationTimeoutListener线程，看样子是用来检测调用超时问题。 在InvokerProcessHandlerFactory的init方法中，就是在service的proxy中，加入了一条责任链。 1234567registerBizProcessFilter(new TraceFilter());registerBizProcessFilter(new DegradationFilter());registerBizProcessFilter(new ClusterInvokeFilter());registerBizProcessFilter(new GatewayInvokeFilter());registerBizProcessFilter(new ContextPrepareInvokeFilter());registerBizProcessFilter(new SecurityFilter());registerBizProcessFilter(new RemoteCallInvokeFilter()); SerializerFactory见下 LoadBalanceManager的init方法，初始化了pigeon支持的四种负载均衡的算法。 RandomLoadBalance AutoawareLoadBalance RoundRobinLoadBalance WeightedAutoawareLoadBalance 在RegionPolicyManager的init方法中，就是对Region的选择算法进行了初始化。 那个monitor的方法，我看好像还没写好。。。。 ProviderBootStrap初始化包含了SerializerFactory和ProviderProcessHandlerFactory的初始化。 同时对jetty server和netty server两个server进行了初始化和启动。 同时启动了一个监听服务器的线程ShutdownHookListener 如果服务器关闭，那么就会调用一些类的关闭方法 1234//从zookeeper上unpublish掉所有的服务ServiceFactory.unpublishAllServices();InvokerBootStrap.shutdown();ProviderBootStrap.shutdown(); 关闭12345678910111213141516171819public static void shutdown() &#123; for (Server server : serversMap.values()) &#123; if (server != null) &#123; logger.info(\"start to stop \" + server); try &#123; unregisterConsoleServer(server.getServerConfig()); server.stop(); &#125; catch (Throwable e) &#123; &#125; if (logger.isInfoEnabled()) &#123; logger.info(server + \" has been shutdown\"); &#125; &#125; &#125; try &#123; ProviderProcessHandlerFactory.destroy(); &#125; catch (Throwable e) &#123; &#125;&#125; 把server从zookeeper上下掉，这个过程调用了RegistryManager的方法。 然后调用了ProviderProcessHandlerFactory的destroy方法。 SerializerFactory对框架支持的序列化方法进行了注册。 内部维护了两个map 12ConcurrentHashMap&lt;String, Byte&gt; serializerTypes;ConcurrentHashMap&lt;Byte, Serializer&gt; serializers 这个Serializer类有啥用呢，这个其实还值得我们去研究研究。 12345678910111213141516public interface Serializer &#123; Object deserializeRequest(InputStream is) throws SerializationException; void serializeRequest(OutputStream os, Object obj) throws SerializationException; Object deserializeResponse(InputStream is) throws SerializationException; void serializeResponse(OutputStream os, Object obj) throws SerializationException; Object proxyRequest(InvokerConfig&lt;?&gt; invokerConfig) throws SerializationException; InvocationResponse newResponse() throws SerializationException; InvocationRequest newRequest(InvokerContext invokerContext) throws SerializationException;&#125; Serializer接口中有七个方法，都是和序列化有关，其中proxyRequest方法请看pigeon的服务注册与发布. 我们进行方法调用，发送请求，获取调用结果。这中间的过程都是需要进行序列化的。 pigeon支持11种序列化方法 123456789101112131415161718192021public enum SerializerType &#123; INTERNAL_THRIFT((byte) 1, \"internalThrift\"), HESSIAN((byte) 2, \"hessian\"), JAVA((byte) 3, \"java\"), PROTO((byte) 5, \"proto\"), HESSIAN1((byte) 6, \"hessian1\"), JSON((byte) 7, \"json\"), FST((byte) 8, \"fst\"), PROTOBUF((byte) 9, \"protobuf\"), THRIFT((byte) 10, \"thrift\"), PROTOBUF3((byte) 11, \"protobuf3\"); 同时有八个进行序列化操作的实体类。 ProviderProcessHandlerFactory我们知道在InvokerProcessHandlerFactory中，给invoke的过程加上了一天责任链。 那么这个的init方法就是给Provide的时候，也加上一条责任链。 12345678910registerBizProcessFilter(new TraceFilter());if (Constants.MONITOR_ENABLE) &#123; registerBizProcessFilter(new MonitorProcessFilter());&#125;registerBizProcessFilter(new WriteResponseProcessFilter());registerBizProcessFilter(new ContextTransferProcessFilter());registerBizProcessFilter(new ExceptionProcessFilter());registerBizProcessFilter(new SecurityFilter());registerBizProcessFilter(new GatewayProcessFilter());registerBizProcessFilter(new BusinessProcessFilter());","link":"/2018/02/18/pigeon中的核心类和作用/"},{"title":"spi和破坏双亲委派","text":"Java SPIJava的spi约定是在源代码目录下的META-INF/services中，新建一个文件，文件名是接口的全限定名，内容是我们想要进行动态Loader的类的全限定名。 比如我们有一个NameLog的接口 123456package cc.lovezhy.spi; public interface NameLog &#123; void printName();&#125; 然后我们设定了两个实现FirstNameLog和SecondNameLog12345678package cc.lovezhy.spi; public class FirstNameLog implements NameLog &#123; @Override public void printName() &#123; System.out.println(\"here is First\"); &#125;&#125; 12345678package cc.lovezhy.spi; public class SecondNameLog implements NameLog &#123; @Override public void printName() &#123; System.out.println(\"here is second\"); &#125;&#125; 在源码的目录下新建META-INF/services目录再新建一个名字为cc.lovezhy.spi.NameLog的文件在里面写上1cc.lovezhy.spi.FirstNameLog 测试程序12345678public class Main &#123; public static void main(String[] args) &#123; ServiceLoader&lt;NameLog&gt; services = ServiceLoader.load(NameLog.class); for (NameLog nameLog : services) &#123; nameLog.printName(); &#125; &#125;&#125; 结果：here is First 一样的，如果我们改成1cc.lovezhy.spi.SecondNameLog 那么就是打印出here is second 我们也可以同时写上就会两个都拿到12here is Firsthere is second 我一开始想的是运行时的实现，后来发现不是，运行时改了时候似乎并没有重新Load 在最近看的pigeon的源代码中其实也看到了很多这样的用法。 经典用法莫过于经常提到的jdbc驱动了，我们只要引入了数据库的jar包，就可以自动找到数据库驱动。 破坏双亲委派这也是意外在某个文章里看到的，之前看Java虚拟机书的时候也看到了，但是没太注意。 双亲委派jvm的类加载机制，最经典的就是双亲委派机制。jvm类加载器，系统中一般有三个 BootstrapClassloader这个是虚拟机层次的，对用户是不可见的，lang包中的类一般是由这个加载器加载。如果我们输出String.class.getClassLoader()会得到null。 ExtClassloader 一般是JAVA_HOME/jre/lib/ext/目录下的 AppClassloader 一般我们自定义的类都是由这个加载器加载。 JVM中类确认需要两个条件 加载它的类加载器 它的全路径名也就是说，及时两个类类名一样，但是加载它们的类加载器不同，那么还是两个类。 双亲委派就是一个加载器加载一个类的时候，会先让父类加载，如果父类加载不了，才自己加载。这样就可以保证这个类在虚拟机中不被重复加载，还有唯一性。 还有一个规则，如果一个类引用了另外一个类，那么这两个类的类加载器就必须是一样的。 那么问题就来了，BootStrapClassLoader并不能认识我们的实现的第三方代码。那怎么办呢？ 如果我们看ClassLoader的类加载器，会发现它是null的，也就是它是由BootstrapClassloader加载的。但是再去看我们那些实现类的类加载器，会发现它们是由AppClassloader加载的。 这是怎么做到的呢，这里需要引入线程上下文加载器。我们可以通过这样得到线程上下文类加载器。123456789public static void main(String[] args) throws ClassNotFoundException &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"hello\"); &#125; &#125;); System.out.println(thread.getContextClassLoader());&#125; 输出结果为sun.misc.Launcher$AppClassLoader@6b482747 由此可见，虽然Thread的类加载器为BootstrapClassloader加载的，但是它的上下文类加载器却是AppClassLoader。 从ServiceLoader源码中也可以看到12345678public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) &#123; ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl);&#125;public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service, ClassLoader loader)&#123; return new ServiceLoader&lt;&gt;(service, loader);&#125;","link":"/2018/03/03/spi和破坏双亲委派/"},{"title":"pigeon的服务注册与发现","text":"前言pigeon是大众点评内部一直在使用的rpc框架，同时带有服务治理的功能。组件是使用zookeeper + netty + jetty完成。现在也已经开源 pigeon 但是开源似乎做的不用心，可能也是看dubbo很火热，pigeon没人用而且pigeon和点评内部的一点闭源的框架联系紧密，所以用的人并不多(反正我是一个没看到)。 使用demoServer端pigeon是与spring进行了很重的耦合的，如果要使用pigeon，那么必须使用spring框架。 如果要暴露我们的服务比如创建一个简单的UserService接口和实现。 12345678910public interface UserService &#123; String getNameById();&#125; public class UserServiceImpl implements UserService &#123; @Override public String getNameById() &#123; return \"id + name\"; &#125;&#125; 由于pigeon的服务注册和发现是依赖zookeeper的，我们还需要装载一个zookeeper在resources文件夹下创建config文件夹，在config文件夹下创建pigeon.properties文件在其中写上zookeeper的ip和端口1pigeon.registry.address=localhost:2181 同时为了区分服务，需要给我们的服务起一个名字在sources的META-INF文件夹的app.properties里写上1pigeon.registry.address=localhost:2181 如果使用typical的方法声明服务的话，我们创建一个spring的配置文件，假设就叫spring.xml12345678910&lt;bean id=\"userServiceImpl\" class=\"cc.lovezhy.service.UserServiceImpl\"/&gt; &lt;bean class=\"com.dianping.pigeon.remoting.provider.config.spring.ServiceBean\" init-method=\"init\"&gt; &lt;property name=\"services\"&gt; &lt;map&gt; &lt;entry key=\"cc.lovezhy.service.UserService\" value-ref=\"userServiceImpl\"/&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt; 这时候，就可以直接用Main方法启动了1234567public class Main &#123; public static void main(String[] args) throws Exception &#123; SpringContainer container = new SpringContainer(\"classpath*:/META-INF/spring.xml\"); container.start(); System.in.read(); &#125;&#125; 当然如果你想要部署到tomcat中也是可以的。 同时默认在localhost:4080/services上我们可以看到当前我们暴露出去的服务并且可以调用。 client端client端只需要包含服务的接口就行了。所以service端的代码编写都是分模块写的，一个模块专门提供DTO和service接口打包成maven供其他人使用。然后具体的实现我们再在另外一个模块里写。 resources和META-INF的内容和服务端一样的。就是在bean的声明的时候，我们需要这样1234&lt;bean id=\"userService\" class=\"com.dianping.pigeon.remoting.invoker.config.spring.ReferenceBean\" init-method=\"init\"&gt; &lt;property name=\"interfaceName\" value=\"cc.lovezhy.service.UserService\"/&gt; &lt;/bean&gt; 然后我们就可以在Main方法中引用了 1234567public class Main &#123; public static void main(String[] args) &#123; ApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"classpath*:spring.xml\"); UserService userService = applicationContext.getBean(UserService.class); System.out.println(userService.getNameById()); &#125;&#125; 配置客户端调用模式在pigeon内部，客户端调用远程服务有4种模式 sync同步 future异步 callback也是异步，只是通过回调的方式来处理结果 oneway不需要回复 例如spring编程方式下只需要配置callType属性： 123456&lt;bean id=\"babyAccountService\" class=\"com.dianping.pigeon.remoting.invoker.config.spring.ReferenceBean\" init-method=\"init\"&gt; &lt;property name=\"interfaceName\" value=\"com.dianping.babytech.casecenter.api.BabyAccountService\"/&gt; &lt;property name=\"serialize\" value=\"hessian\" /&gt; &lt;property name=\"callType\" value=\"sync\"/&gt; &lt;property name=\"timeout\" value=\"5000\"/&gt; &lt;/bean&gt; 官方文档解释的很清楚了所有的选项。1234567891011121314151617181920&lt;bean id=\"echoService\" class=\"com.dianping.pigeon.remoting.invoker.config.spring.ReferenceBean\" init-method=\"init\"&gt; &lt;!-- 服务全局唯一的标识url，默认是服务接口类名，必须设置 --&gt; &lt;property name=\"url\" value=\"http://service.dianping.com/demoService/echoService_1.0.0\" /&gt; &lt;!-- 接口名称，必须设置 --&gt; &lt;property name=\"interfaceName\" value=\"com.dianping.pigeon.demo.EchoService\" /&gt; &lt;!-- 超时时间，毫秒，默认5000，建议自己设置 --&gt; &lt;property name=\"timeout\" value=\"2000\" /&gt; &lt;!-- 序列化，hessian/fst/protostuff，默认hessian，可不设置--&gt; &lt;property name=\"serialize\" value=\"hessian\" /&gt; &lt;!-- 调用方式，sync/future/callback/oneway，默认sync，可不设置 --&gt; &lt;property name=\"callType\" value=\"sync\" /&gt; &lt;!-- 失败策略，快速失败failfast/失败转移failover/失败忽略failsafe/并发取最快返回forking，默认failfast，可不设置 --&gt; &lt;property name=\"cluster\" value=\"failfast\" /&gt; &lt;!-- 是否超时重试，默认false，可不设置 --&gt; &lt;property name=\"timeoutRetry\" value=\"false\" /&gt; &lt;!-- 重试次数，默认1，可不设置 --&gt; &lt;property name=\"retries\" value=\"1\" /&gt;&lt;/bean&gt; ReferenceBean的获取，init方法这个我也画了一个简单的图，不过省略了很多细节部分，在InvokerBootStrap部分和Serializer.proxyRequest部分省略了很多。 下面具体看代码。 在我们声明bean的时候还带有一个init-method参数init-method=&quot;init&quot;意思是在这个bean创建的时候还会调用一下ReferenceBean的init方法。 在init方法中 123456789101112131415161718192021222324252627public void init() throws Exception &#123; this.objType = ClassUtils.loadClass(this.classLoader, this.interfaceName.trim()); //这里直接就创建了，其实如果配置简单的话，很多的参数都是空的。 InvokerConfig&lt;?&gt; invokerConfig = new InvokerConfig(this.objType, this.url, this.timeout, this.callType, this.serialize, this.callback, this.group, this.writeBufferLimit, this.loadBalance, this.cluster, this.retries, this.timeoutRetry, this.vip, this.version, this.protocol); invokerConfig.setClassLoader(classLoader); invokerConfig.setSecret(secret); invokerConfig.setRegionPolicy(regionPolicy); if (!CollectionUtils.isEmpty(methods)) &#123; Map&lt;String, InvokerMethodConfig&gt; methodMap = new HashMap&lt;String, InvokerMethodConfig&gt;(); invokerConfig.setMethods(methodMap); for (InvokerMethodConfig method : methods) &#123; methodMap.put(method.getName(), method); &#125; &#125; //降级配置检查 checkMock(); invokerConfig.setMock(mock); checkRemoteAppkey(); invokerConfig.setRemoteAppKey(remoteAppKey); //这里就得到了service的代理实例。 this.obj = ServiceFactory.getService(invokerConfig); configLoadBalance(invokerConfig);&#125; 123InvokerConfig&lt;?&gt; invokerConfig = new InvokerConfig(this.objType, this.url, this.timeout, this.callType, this.serialize, this.callback, this.suffix, this.writeBufferLimit, this.loadBalance, this.cluster, this.retries, this.timeoutRetry, this.vip, this.version, this.protocol); InvokerConfig参数 Class&lt;T&gt; serviceInterface 接口的Class类 String url 服务全局唯一的标识url，感觉有了serviceInterface就够了 String version 版本？ byte callMethod 就是call的方式，sync还是future之类，但是是byte类型的，1代表sync等 String callType callType就是callMethod的String byte serialize 序列化方式，默认是hessian int timeout 超时时间 InvocationCallback callback 设置了callback模式才有 String suffix String loadbalance 负载均衡的策略设置 String routePolicy 路由的规则 RoutePolicy routePolicyObj boolean timeoutRetry String cluster int retries 失败了重试的次数 String vip int maxRequests String protocol Map&lt;String, InvokerMethodConfig&gt; methods ClassLoader classLoader String secret String remoteAppKey Object mock 上面还提到一个服务降级的问题，这个我们之后再说。 从上面看到，得到的service是从ServiceFactory这个类直接得到的。 在getService的时候直接get了 123456789101112131415161718//ServiceFactory这个类，主要就是管理service的加载，发布和获取的。//它的很多方法，从名字就可以看出来，发布service，取消service，获得service，给service设置权重之类。public class ServiceFactory &#123; private static ServiceProxy serviceProxy = ServiceProxyLoader.getServiceProxy(); private static PublishPolicy publishPolicy = PublishPolicyLoader.getPublishPolicy(); static &#123; try &#123; //这个类，从名字上就可以看到和Provider相关的，进行了一些东西的初始化 ProviderBootStrap.init(); &#125; catch (Throwable t) &#123; logger.error(\"error while initializing service factory:\", t); System.exit(1); &#125; &#125; public static &lt;T&gt; T getService(InvokerConfig&lt;T&gt; invokerConfig) throws RpcException &#123; return serviceProxy.getProxy(invokerConfig); &#125;&#125; 默认的话是建一个DefaultServiceProxy它的调用是调用了父类的123456public final class DefaultServiceProxy extends AbstractServiceProxy &#123; @Override public &lt;T&gt; T getProxy(InvokerConfig&lt;T&gt; invokerConfig) &#123; return super.getProxy(invokerConfig); &#125;&#125; 在AbstractServiceProxy中1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//这个其实是个缓存的map，看下面的get过程中有个加锁的过程protected final static Map&lt;InvokerConfig&lt;?&gt;, Object&gt; services = new ConcurrentHashMap&lt;InvokerConfig&lt;?&gt;, Object&gt;();@Overridepublic &lt;T&gt; T getProxy(InvokerConfig&lt;T&gt; invokerConfig) &#123; //...省略好多好多 Object service = null; service = services.get(invokerConfig); if (service == null) &#123; //这个锁，好像用的guava中一个东西，提供和String.intern相同的作用但是不会占用老年代空间？ //有时间再研究研究 synchronized (interner.intern(invokerConfig)) &#123; service = services.get(invokerConfig); if (service == null) &#123; try &#123; //参见另外一篇文章 InvokerBootStrap.startup(); //下面这句就调用AbstractSerializer中的proxyRequest动态生成一个代理类 service = SerializerFactory.getSerializer(invokerConfig.getSerialize()).proxyRequest(invokerConfig); if (StringUtils.isNotBlank(invokerConfig.getLoadbalance())) &#123; LoadBalanceManager.register(invokerConfig.getUrl(), invokerConfig.getSuffix(), invokerConfig.getLoadbalance()); &#125; &#125; catch (Throwable t) &#123; throw new RpcException(\"error while trying to get service:\" + invokerConfig, t); &#125; try &#123; //配置地域策略，默认的是autoSwitch routePolicyManager.register(invokerConfig.getUrl(), invokerConfig.getSuffix(), invokerConfig.getRoutePolicy()); &#125; catch (Throwable t) &#123; throw new RouteException(\"error while setup region route policy: \" + invokerConfig, t); &#125; // watch service config try &#123; serviceConfigManager.register(invokerConfig.getUrl()); &#125; catch (Throwable t) &#123; throw new ConfigException(\"error while trying to watch service config: \" + invokerConfig, t); &#125; try &#123; ClientManager.getInstance().registerClients(invokerConfig); &#125; catch (Throwable t) &#123; logger.warn(\"error while trying to setup service client:\" + invokerConfig, t); &#125; //加到Map缓存里 services.put(invokerConfig, service); &#125; &#125; &#125; return (T) service;&#125; 在InvokerBootStrap.startup()中123456789101112131415161718192021//有很多的init方法，说明都是和invoke相关的一些配件。 //这是处理调用超时问题的，把一个InvocationTimeoutListener跑在一个线程中ServiceInvocationRepository.getInstance().init(); //初始化那个proxy的handle，下面会提到InvokerProcessHandlerFactory.init(); //序列化工厂的初始化，默认支持很多序列化方式SerializerFactory.init(); //负载均衡调度的初始化，默认支持四种LoadBalanceManager.init(); //Region策略，就是分地域的策略，如果是北京上海都有服务，那么调用哪一边的问题。RegionPolicyManager.INSTANCE.init();Monitor monitor = MonitorLoader.getMonitor();if (monitor != null) &#123; monitor.init();&#125;isStartup = true; 得到服务的唯一标志，如果不指定url的话，默认就是接口的全称。12345ServiceFactory.getServiceUrl(invokerConfig);public static &lt;T&gt; String getServiceUrl(InvokerConfig&lt;T&gt; invokerConfig) &#123; String url = invokerConfig.getServiceInterface().getCanonicalName(); return url;&#125; service代理对象12345678910//proxyRequest得到一个proxy的对象public abstract class AbstractSerializer implements Serializer &#123; @Override public Object proxyRequest(InvokerConfig&lt;?&gt; invokerConfig) throws SerializationException &#123; //第一个参数是ClassLoader，第二个参数是interface的数组，第三个参数是Proxy类 return Proxy.newProxyInstance(ClassUtils.getCurrentClassLoader(invokerConfig.getClassLoader()), new Class[] &#123; invokerConfig.getServiceInterface() &#125;, new ServiceInvocationProxy(invokerConfig, InvokerProcessHandlerFactory.selectInvocationHandler(invokerConfig))); &#125;&#125; 123456789101112131415161718192021222324252627//这里的handler是连接远程调用的桥梁public class ServiceInvocationProxy implements InvocationHandler &#123; public ServiceInvocationProxy(InvokerConfig&lt;?&gt; invokerConfig, ServiceInvocationHandler handler) &#123; this.invokerConfig = invokerConfig; this.handler = handler; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; String methodName = method.getName(); Class&lt;?&gt;[] parameterTypes = method.getParameterTypes(); if (method.getDeclaringClass() == Object.class) &#123; return method.invoke(handler, args); &#125; if (\"toString\".equals(methodName) &amp;&amp; parameterTypes.length == 0) &#123; return handler.toString(); &#125; if (\"hashCode\".equals(methodName) &amp;&amp; parameterTypes.length == 0) &#123; return handler.hashCode(); &#125; if (\"equals\".equals(methodName) &amp;&amp; parameterTypes.length == 1) &#123; return handler.equals(args[0]); &#125; return extractResult(handler.handle(new DefaultInvokerContext(invokerConfig, methodName, parameterTypes, args)), method.getReturnType()); &#125;&#125; 当我们调用其他的方法时，其实是在1handler.handle(new DefaultInvokerContext(invokerConfig, methodName, parameterTypes, args) 这里面进行处理的。 这里的handle是1private ServiceInvocationHandler handler; 在InvokerProcessHandlerFactory中得到一个实例//这里应该是最重要的部分了12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public final class InvokerProcessHandlerFactory &#123; private static List&lt;InvocationInvokeFilter&gt; bizProcessFilters = new LinkedList&lt;InvocationInvokeFilter&gt;(); private static ServiceInvocationHandler bizInvocationHandler = null; private static volatile boolean isInitialized = false; //这里进行了一个责任链的处理，类似于netty的那种 //在调用前进行日志，权限之类的分析。 //最后的RemoteCallInvokeFilter才是真正调用我们想要的方法 //初始化在之前就进行了，见上 public static void init() &#123; if (!isInitialized) &#123; if (Constants.MONITOR_ENABLE) &#123; registerBizProcessFilter(new RemoteCallMonitorInvokeFilter()); &#125; //trace 监控信息 registerBizProcessFilter(new TraceFilter()); //服务降级 registerBizProcessFilter(new DegradationFilter()); registerBizProcessFilter(new ClusterInvokeFilter()); //网关，统计流量啥的 registerBizProcessFilter(new GatewayInvokeFilter()); registerBizProcessFilter(new ContextPrepareInvokeFilter()); //安全验证 registerBizProcessFilter(new SecurityFilter()); //通过Netty调用 registerBizProcessFilter(new RemoteCallInvokeFilter()); bizInvocationHandler = createInvocationHandler(bizProcessFilters); isInitialized = true; &#125; &#125; public static ServiceInvocationHandler selectInvocationHandler(InvokerConfig&lt;?&gt; invokerConfig) &#123; return bizInvocationHandler; &#125; @SuppressWarnings(&#123; \"rawtypes\" &#125;) private static &lt;V extends ServiceInvocationFilter&gt; ServiceInvocationHandler createInvocationHandler( List&lt;V&gt; internalFilters) &#123; ServiceInvocationHandler last = null; List&lt;V&gt; filterList = new ArrayList&lt;V&gt;(); filterList.addAll(internalFilters); //创建一个调用链 for (int i = filterList.size() - 1; i &gt;= 0; i--) &#123; final V filter = filterList.get(i); final ServiceInvocationHandler next = last; last = new ServiceInvocationHandler() &#123; @SuppressWarnings(\"unchecked\") @Override public InvocationResponse handle(InvocationContext invocationContext) throws Throwable &#123; InvocationResponse resp = filter.invoke(next, invocationContext); return resp; &#125; &#125;; &#125; return last; &#125; public static void registerBizProcessFilter(InvocationInvokeFilter filter) &#123; bizProcessFilters.add(filter); &#125;&#125; 在RemoteCallInvokeFilter中invoke方法中，调用的是InvokerUtils的方法1response = InvokerUtils.sendRequest(client, invocationContext.getRequest(), future); 123456789101112131415161718192021222324public static InvocationResponse sendRequest(Client client, InvocationRequest request, Callback callback) &#123; if (request.getCallType() == Constants.CALLTYPE_REPLY) &#123; RemoteInvocationBean invocationBean = new RemoteInvocationBean(); invocationBean.request = request; invocationBean.callback = callback; callback.setRequest(request); callback.setClient(client); invocationRepository.put(request.getSequence(), invocationBean); &#125; InvocationResponse response = null; try &#123; //发送的请求在这儿发送的 response = client.write(request); &#125; catch (NetworkException e) &#123; invocationRepository.remove(request.getSequence()); logger.warn(\"network exception ocurred:\" + request, e); throw e; &#125; finally &#123; if (response != null) &#123; invocationRepository.remove(request.getSequence()); &#125; &#125; return response;&#125; client的实现有两种，一个是Tcp的还有一个是Http的，pigeon两种都支持。好像一般的调用是调用的tcp的方案，然后在4080/services查看和调用的是http的协议。 在Netty的实现中的doWrite方法，其实就是调用了channel的write0方法12345678910111213141516@Overridepublic InvocationResponse doWrite(InvocationRequest request) throws NetworkException &#123; NettyChannel channel = null; try &#123; channel = channelPool.selectChannel(); ChannelFuture future = channel.write0(request); afterWrite(request, channel); if (request.getMessageType() == Constants.MESSAGE_TYPE_SERVICE || request.getMessageType() == Constants.MESSAGE_TYPE_HEART) &#123; future.addListener(new MessageWriteListener(request, channel)); &#125; &#125; catch (Exception e) &#123; throw new NetworkException(\"[doRequest] remote call failed:\" + request, e); &#125; return null;&#125; 至此，代理bean的创建和invoke的流程大概就理清楚了。 但是其实它是怎么和zookeeper沟通拿到service的呢。 我猜想肯定是从那条责任链的某个地方中取得的，于是进行了一番苦苦查找。 在ClusterFactory的select中，默认是返回FailfastCluster，在它的invoke方法中 1Client remoteClient = clientManager.getClient(invokerConfig, request, null); 有这句话。 这个Client默认是NettyClient。 如果我们在仔细看一下ClientManager的构造函数 12345678910private ClientManager() &#123; this.providerAvailableListener = new ProviderAvailableListener(); this.clusterListener = new DefaultClusterListener(providerAvailableListener); this.clusterListenerManager.addListener(this.clusterListener); providerAvailableThreadPool.execute(this.providerAvailableListener); RegistryEventListener.addListener(providerChangeListener); RegistryEventListener.addListener(registryConnectionListener); RegistryEventListener.addListener(groupChangeListener); registerThreadPool.getExecutor().allowCoreThreadTimeOut(true);&#125; 这里其实启动了一个ProviderAvailableListener。 这个类和RegisterManager关系密切，而RegisterManager则是掌管和zookeeper沟通的，由此不难看出，这里的service就是从这儿得到的。 服务端集群策略在ClusterInvokeFilter中进行的配置 failfast - 调用服务的一个节点失败后抛出异常返回，可以同时配置重试timeoutRetry和retries属性 failover - 调用服务的一个节点失败后会尝试调用另外的一个节点，可以同时配置重试 timeoutRetry和retries属性 failsafe - 调用服务的一个节点失败后不会抛出异常，返回null，后续版本会考虑按配置默认值返回 forking - 同时调用服务的所有可用节点，返回调用最快的节点结果数据。可以通过配置forkingSize，指定最多调用的节点数（pigeon2.10.3及以上版本通过xml配置forkingSize，其余版本可以通过lion配置{appkey}.pigeon.invoker.forking.size） hedged - 发出第一个请求后，如果hedgedDelay时间内没有返回，会向其他节点发送第二个请求，返回最先返回的结果数据 ServiceBean的注册（画了我好久的图）具体的流程和ReferenceBean的获取其实差不了太多。","link":"/2018/02/18/pigeon的服务注册与发现/"},{"title":"使用AuthenticationProvider进行自定义登陆配置","text":"我们在使用spring sec的时候，一般会继承WebSecurityConfigurerAdapter类然后选择覆盖protected void configure(AuthenticationManagerBuilder auth)protected void configure(HttpSecurity http)方法 1234567891011121314@Overrideprotected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.userDetailsService(studentService).passwordEncoder(encoder); auth.userDetailsService(teacherService).passwordEncoder(encoder);&#125;@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http.csrf().disable() .authorizeRequests() .antMatchers(\"/**\").authenticated() .and() .formLogin().loginPage(\"/login\").permitAll();&#125; 一般而言登录的数据我们在protected void configure(AuthenticationManagerBuilder auth)中，我们在studentService中配置一个 123456@Overridepublic UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; Student student = studentRepository.findByStudentId(username) .orElseThrow(() -&gt; new UsernameNotFoundException(\"Not found: \" + username)); return new StudentDetails(student); &#125; 方法就好。 但是遇到一个问题，这样的话用户名和密码都是定死的，我们拿不到form-data数据，如果因为前端的问题，这种密码登录方式以外，我们还要稍微修改提交给我们的form-data中的密码数据，做一下处理，自定义一个登录呢。 这个时候就需要用到AuthenticationProvider了。这是一个接口，提供了两种方法 123456public interface AuthenticationProvider &#123; Authentication authenticate(Authentication authentication) throws AuthenticationException; boolean supports(Class&lt;?&gt; authentication);&#125; 通过第一个方法我们可以拿到form-data的数据，并且返回一个UserDetails如果登录成功的话，或者返回null如果登录失败。 123456789101112@Overridepublic Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; String userName = (String) authentication.getPrincipal(); //拿到username String password = (String) authentication.getCredentials(); //拿到password UserDetails userDetails = studentService.loadUserByUsername(userName); if (/*自定义的验证通过*/) &#123; return new UsernamePasswordAuthenticationToken(userDetails, null,userDetails.getAuthorities()); &#125; /*验证不通过*/ return null;&#125; 第二个方法是告诉spring sec我们这个验证支持哪种验证。 这种验证属于Dao验证。也就是DaoAuthenticationProvider 也就是UsernamePasswordAuthentication。所以在第二个方法中一般会这么写 1234@Overridepublic boolean supports(Class&lt;?&gt; authentication) &#123; return authentication.equals(UsernamePasswordAuthenticationToken.class);&#125; 下面就是注册要configure方法中了只要加入 1auth.authenticationProvider(new MyAuthenticationProvider()); 就好了。。","link":"/2017/05/28/use_AuthenticationProvider_in_springSec/"},{"title":"Vim技巧","text":"前言其实我们不一定所有配置都要写在.vimrc里面的偶尔在vim的命令行里执行一些小程序也是很方便的 改tab为space12:set expandtab:%retab","link":"/2018/04/01/vim技巧/"},{"title":"哪些可以作为GC Roots","text":"主要的GC算法 引用计数不赘述了，主要是无法解决互相引用的问题 可达性分析就是从一些叫做GC Roots的引用开始，一步一步的找能达到的对象，如果没达到，那么就不可达的，就会开始回收它。 HotSpot采用的是可达性分析。那么哪些可以作为GC Roots呢？ 书上的 在《深入理解Java虚拟机-JVM高级特性与最佳实践》中，简单的提了一下 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI（native方法）引用的对象 简单解释下 第一种就是指当前的Java线程中正在调用的方法的引用类型的参数，局部变量，临时值，这个具有随机性。 第二种就是指类对象的引用对象，比如123public class Solution &#123; private static List&lt;String&gt; list = new LinkedList&lt;&gt;();&#125; 这里的list就是这种。 第三种是指的常量一般指static final这种，比如123public class Solution &#123; public static final String name = \"hello\";&#125; 这里的name就是，当然枚举类中的也是。 第四种就是native方法了，就是一般是c语言或者c++写的。 再进行拓展 我又去搜了一下。 又找到了更具体的一下，其实哪些能作为GC Roots在一定程度上也是依赖收集器的设计的。 来自RednaxelaFX的答案 （看情况）所有当前被加载的Java类 （看情况）Java类的引用类型静态变量 （看情况）Java类的运行时常量池里的引用类型常量（String或Class类型） 这里还多了Class类型，。。。。 在eclipse的文档网站，找到了更为详细的 eclipse System Class被bootstrap类加载器加载的类 JNI LocalJNI定义的局部变量 JNI GlobalJNI定义的全局变量 Thread Block当前线程块中的对象 Thread未停止的线程 Busy Monitor调用了wait()或者notify()或者正在进行同步的对象和类 Java Local当前线程虚拟机栈中的本地变量 Native Stack本地方法栈中的对象 Finalizable一个正在在等待finalize的对象 Unfinalized拥有finalize方法，但是不在等待finalize的对象 Unreachable一个不可达的对象，但是仍然被Memory Analyzer标记为GC Root的对象这个的解释就是对于一些小对象，GC觉得收集它太浪费珍贵的GC时间了，就放弃了。 Java Stack FrameJava栈帧，如果你设定栈帧也为一个对象的话 Unknown其他未知的 总结 上面说的很多重复的，有些是看不懂的。所以我自己总结了下 所有线程当前虚拟机栈（栈帧中的本地变量表）中引用的对象，包括局部变量，参数对象。 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI（native方法）引用的对象，包括局部变量和全局变量 被bootstrap类加载器加载的类 当前所有活跃的线程 调用了wait()或者notify()或者正在进行同步的对象和类 一个正在在等待finalize的对象 拥有finalize方法，但是不在等待finalize的对象 一个不可达的对象，但是仍然被Memory Analyzer标记为GC Root的对象","link":"/2017/10/24/哪些可以作为GCRoots/"},{"title":"叶子","text":"去帮室友拿快递的时候看到的。暑假没人打扫。落了满地。","link":"/2017/07/26/叶子/"},{"title":"动态代理的缓存实现","text":"字节码 我们经常听到字节码，简单的说字节码就是Java源代码到虚拟机执行的期间生成的中间代码。 JVM虽然叫Java虚拟机，但是更准确的叫法我觉得应该是字节码虚拟机。 因为我们听过很多的JVM语言，其实它们之所以能在JVM上运行，也是编译成字节码丢进去运行的。 我们一般使用javac来生成java字节码来让jvm去运行。 那其实宽泛的说javac也就是一个字节码生成器。 因为字节码是二进制流，所以我们自然也可以用java语言手动产生一个字节码流。 字节码生成器 字节码生成器说到底就是根据字节码规范去用程序生成字节码。 一般用的比较多的就是Javaassist，CGLIB之类。 提到CGLIB我们很容易想到的就是代理。 不过在java的动态代理的实现中，也是利用这种方法自动产生动态类加载进去的。 动态代理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//声明一个接口public interface Greeting &#123; void sayHello(); void sayWorld();&#125; public class Student implements Greeting &#123; @Override public void sayHello() &#123; System.out.println(\"hello\"); &#125; @Override public void sayWorld() &#123; System.out.println(\"world\"); &#125;&#125;class StudentInvocationHandler implements InvocationHandler &#123; private Object object; public StudentInvocationHandler(Object o) &#123; this.object = o; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if (method.getName().equals(\"sayHello\")) &#123; System.out.println(\"beforeSayHello\"); &#125; else if (method.getName().equals(\"sayWorld\")) &#123; System.out.println(\"beforeSayWorld\"); &#125; return method.invoke(object,args); &#125;&#125;public class Aop &#123; public static void main(String[] args) &#123; Student student = new Student(); StudentInvocationHandler studentAop = new StudentInvocationHandler(student); Greeting greeting = (Greeting) Proxy.newProxyInstance( student.getClass().getClassLoader(), student.getClass().getInterfaces(), studentAop); greeting.sayHello(); greeting.sayWorld(); &#125;&#125; 运行结果是 beforeSayHellohellobeforeSayWorldworld 那么大概动态代理帮我们生成了什么代理类呢 声明大概是这样 123public class ProxyClass extends Students implements Greeting &#123; &#125; 并且把其中的每个方法的内容大概都改为了 public Object invoke(Object proxy, Method method, Object[] args) throws Throwable 的内容。 Proxy类 123456789101112131415161718192021222324252627public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException&#123; // ... Class&lt;?&gt; cl = getProxyClass0(loader, intfs); try &#123; final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; return cons.newInstance(new Object[]&#123;h&#125;); &#125; catch ( ... ) &#123; //... &#125;&#125; 进行了一些缩减，try里面的代码是对新的类进行了newInstance，所以在getProxyClass0中就已经生成好了新的代理类并进行了加载。 1234567private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) &#123; if (interfaces.length &gt; 65535) &#123; throw new IllegalArgumentException(\"interface limit exceeded\"); &#125; return proxyClassCache.get(loader, interfaces);&#125; 这里的proxyClassCache，在类中定义为，保存的是代理Class的缓存 12private static final WeakCache&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; proxyClassCache = new WeakCache&lt;&gt;(new KeyFactory(), new ProxyClassFactory()); 简单说一下这个WeakCache的作用，在这儿之前，先看看new的两个类分别是做什么用的？ 先看KeyFactory123456789101112131415161718192021222324252627282930/* * 这个类是一个函数式类，实现了BiFunction，实现了apply方法 * 其中Key1，Key2和KeyX类都是大概是下面这样，选取了KeyX * private final int hash; * private final WeakReference&lt;Class&lt;?&gt;&gt;[] refs; * KeyX(Class&lt;?&gt;[] interfaces) &#123; * hash = Arrays.hashCode(interfaces); * refs = (WeakReference&lt;Class&lt;?&gt;&gt;[])new WeakReference&lt;?&gt;[interfaces.length]; * for (int i = 0; i &lt; interfaces.length; i++) &#123; * refs[i] = new WeakReference&lt;&gt;(interfaces[i]); * &#125; * 就是说把构造参数整体取个Hash存在hash属性中，然后把每个interface都放到一个弱引用中 * 如果你知道java中的引用级别的话，就知道弱引用的对象只能活到下次gc前。 * 其中key0是这么写的 * private static final Object key0 = new Object(); * */private static final class KeyFactory implements BiFunction&lt;ClassLoader, Class&lt;?&gt;[], Object&gt;&#123; @Override public Object apply(ClassLoader classLoader, Class&lt;?&gt;[] interfaces) &#123; switch (interfaces.length) &#123; case 1: return new Key1(interfaces[0]); // the most frequent case 2: return new Key2(interfaces[0], interfaces[1]); case 0: return key0; default: return new KeyX(interfaces); &#125; &#125;&#125; 再来看看ProxyClassFactory12345678910111213141516171819/** * * 这个类也是实现了一个函数式接口，实现了apply方法，最终的结果就是返回了我们要生成的代理类 **/private static final class ProxyClassFactory implements BiFunction&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt;&#123; // prefix for all proxy class names private static final String proxyClassNamePrefix = \"$Proxy\"; // next number to use for generation of unique proxy class names private static final AtomicLong nextUniqueNumber = new AtomicLong(); @Override public Class&lt;?&gt; apply(ClassLoader loader, Class&lt;?&gt;[] interfaces) &#123; //返回代理类的class &#125;&#125; 看完了参数中的两个类，我们再看看看WeakCache到底怎么实现缓存的，因为直观上看，其实我们觉得这个缓存类非常复杂123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217/** * 我们还是具体问题具体分析，根据上面的构造 * WeakCache&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; * K是ClassLoader， P是接口数组，V是要产生的代理类 * 还是回到最原始的问题上来，既然我们知道这个是做缓存用的，那么最基本的肯定要是Map的key，value的格式 * 根据这个最基本的点，我们来看是怎么做的 */final class WeakCache&lt;K, P, V&gt; &#123; /** * 这个refQueue是用来保存 * 在weak reference指向的对象被回收后, weak reference本身其实也就没有用了. * java提供了一个ReferenceQueue来保存这些所指向的对象已经被回收的reference. * 用法是在定义WeakReference的时候将一个ReferenceQueue的对象作为参数传入构函数. */ private final ReferenceQueue&lt;K&gt; refQueue = new ReferenceQueue&lt;&gt;(); //这里是实际的缓存类，这里配备的是两级缓存 //第一级就是对应的classLoader缓存 //第二级是subKeyFactory.apply(key, parameter)产生的key //supplier#get返回的就是我们需要的代理类class //为什么需要按照classLoader来划分呢，因为双亲委派机制的存在 private final ConcurrentMap&lt;Object, ConcurrentMap&lt;Object, Supplier&lt;V&gt;&gt;&gt; map = new ConcurrentHashMap&lt;&gt;(); private final ConcurrentMap&lt;Supplier&lt;V&gt;, Boolean&gt; reverseMap = new ConcurrentHashMap&lt;&gt;(); private final BiFunction&lt;K, P, ?&gt; subKeyFactory; private final BiFunction&lt;K, P, V&gt; valueFactory; //构造函数，我们传进去了 //new WeakCache&lt;&gt;(new KeyFactory(), new ProxyClassFactory()); //如果你还记得，KeyFactory里面有个字段是对所有的接口做了hash //ProxyClassFactory是返回我们要生成的代理类 public WeakCache(BiFunction&lt;K, P, ?&gt; subKeyFactory, BiFunction&lt;K, P, V&gt; valueFactory) &#123; this.subKeyFactory = Objects.requireNonNull(subKeyFactory); this.valueFactory = Objects.requireNonNull(valueFactory); &#125; /** * WeakCache中还有几个静态内部类，CacheValue和CacheKey */ /** * Value继承了Supplier接口，Supplier也是一个函数式接口，里面有一个get方法 * public interface Supplier&lt;T&gt; &#123; * T get(); * &#125; * CacheValue继承了WeakReference类，也就是弱引用。 * 其中的hash值调用了System.identityHashCode * 这是个native方法，还是返回了hashcode，那为什么不直接调用hashcode方法呢， * 文档中写的是这个方法排除了这个类覆盖了hashcode方法，也就是调用了Object的hashcode方法 */ private interface Value&lt;V&gt; extends Supplier&lt;V&gt; &#123;&#125; private static final class CacheValue&lt;V&gt; extends WeakReference&lt;V&gt; implements Value&lt;V&gt; &#123; private final int hash; CacheValue(V value) &#123; super(value); this.hash = System.identityHashCode(value); &#125; @Override public int hashCode() &#123; return hash; &#125; @Override public boolean equals(Object obj) &#123; V value; return obj == this || obj instanceof Value &amp;&amp; // cleared CacheValue is only equal to itself (value = get()) != null &amp;&amp; value == ((Value&lt;?&gt;) obj).get(); // compare by identity &#125; &#125; /** * CacheKey也是继承了WeakReference方法 */ private static final class CacheKey&lt;K&gt; extends WeakReference&lt;K&gt; &#123; // a replacement for null keys private static final Object NULL_KEY = new Object(); static &lt;K&gt; Object valueOf(K key, ReferenceQueue&lt;K&gt; refQueue) &#123; return key == null // null key means we can't weakly reference it, // so we use a NULL_KEY singleton as cache key ? NULL_KEY // non-null key requires wrapping with a WeakReference : new CacheKey&lt;&gt;(key, refQueue); &#125; private final int hash; private CacheKey(K key, ReferenceQueue&lt;K&gt; refQueue) &#123; super(key, refQueue); this.hash = System.identityHashCode(key); // compare by identity &#125; @Override public int hashCode() &#123; return hash; &#125; @Override public boolean equals(Object obj) &#123; K key; return obj == this || obj != null &amp;&amp; obj.getClass() == this.getClass() &amp;&amp; // cleared CacheKey is only equal to itself (key = this.get()) != null &amp;&amp; // compare key by identity key == ((CacheKey&lt;K&gt;) obj).get(); &#125; void expungeFrom(ConcurrentMap&lt;?, ? extends ConcurrentMap&lt;?, ?&gt;&gt; map, ConcurrentMap&lt;?, Boolean&gt; reverseMap) &#123; // removing just by key is always safe here because after a CacheKey // is cleared and enqueue-ed it is only equal to itself // (see equals method)... ConcurrentMap&lt;?, ?&gt; valuesMap = map.remove(this); // remove also from reverseMap if needed if (valuesMap != null) &#123; for (Object cacheValue : valuesMap.values()) &#123; reverseMap.remove(cacheValue); &#125; &#125; &#125; &#125; /** * 来看看我们的get方法 * 回忆一下我们是怎么调用它的 * proxyClassCache.get(loader, interfaces); * key就是我们的classLoader， parameter是我们的接口数组 */public V get(K key, P parameter) &#123; Objects.requireNonNull(parameter); expungeStaleEntries(); //由于是第一次调用，所以new了一个新的cacheKey出来 Object cacheKey = CacheKey.valueOf(key, refQueue); //由于是第一次调用，所以这里还是等于null /** * 再来分析一下这个map * 这个map的声明是 * ConcurrentMap&lt;Object, ConcurrentMap&lt;Object, Supplier&lt;V&gt;&gt;&gt; map * = new ConcurrentHashMap&lt;&gt;(); * 这三个字段分别是什么意思呢，我们从下面的putIfAbsent()可以看出 * 第一个就是CacheKey，是一个弱引用，下一次gc后不存在了 * 第二个的key是subKeyFactory.apply(key, parameter)产生的keyX对象 * 第二个的value就是产生我们代理类class的Supplier，其中的get方法返回的是代理类的class * 如果我们已经看了下面就会发现第二个value就是CacheValue，也是一个弱引用，下一次gc后不存在了 * 那其实唯一的一个强引用就是keyX对象，但是keyX中的字段中的interfce[]数组还是弱引用的 * 如果经历过一次gc，那唯一存在的就是keyX中的hash值，那其实还是一个过期值。 */ ConcurrentMap&lt;Object, Supplier&lt;V&gt;&gt; valuesMap = map.get(cacheKey); if (valuesMap == null) &#123; ConcurrentMap&lt;Object, Supplier&lt;V&gt;&gt; oldValuesMap = map.putIfAbsent(cacheKey, valuesMap = new ConcurrentHashMap&lt;&gt;()); if (oldValuesMap != null) &#123; valuesMap = oldValuesMap; &#125; &#125; //这里利用KeyFactory返回了一个KeyX对象 Object subKey = Objects.requireNonNull(subKeyFactory.apply(key, parameter)); //得到class的supplier Supplier&lt;V&gt; supplier = valuesMap.get(subKey); Factory factory = null; //这里是个循环，循环退出的条件是supplier.get()返回一个非null的值，这个值就是我们代理类的class //这里的设计是考虑了并发的情况的 //factory的get方法是用来计算出class文件，然后放到缓存中 while (true) &#123; if (supplier != null) &#123; //如果得到了value，那么就返回 //这里有两种情况 //第一种就是已经在缓存中了，所以直接返回就好 //第二种就是缓存中没有它，这里的supplier其实已经被替换成了一个Factory(见下面的代码) //然后调用factory的get方法同步得到一个class V value = supplier.get(); if (value != null) &#123; return value; &#125; &#125; //懒加载一个Factory if (factory == null) &#123; factory = new Factory(key, parameter, subKey, valuesMap); &#125; //反复检查supplier是不是空 if (supplier == null) &#123; supplier = valuesMap.putIfAbsent(subKey, factory); if (supplier == null) &#123; supplier = factory; &#125; &#125; else &#123; if (valuesMap.replace(subKey, supplier, factory)) &#123; supplier = factory; &#125; else &#123; //如果已经有线程进行了处理并加到了缓存中 supplier = valuesMap.get(subKey); &#125; &#125; &#125;&#125; 下面看看Factory到底实现了什么1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 从上面用法看这个Factory最终赋给了Supplier，所以这个类也是提供代理类class的 * 不过这个类的get方法比较特殊，是个同步方法 */private final class Factory implements Supplier&lt;V&gt; &#123; private final K key; private final P parameter; private final Object subKey; private final ConcurrentMap&lt;Object, Supplier&lt;V&gt;&gt; valuesMap; Factory(K key, P parameter, Object subKey, ConcurrentMap&lt;Object, Supplier&lt;V&gt;&gt; valuesMap) &#123; this.key = key; this.parameter = parameter; this.subKey = subKey; this.valuesMap = valuesMap; &#125; @Override //这个get方法是个同步的方法，用来保证在多线程环境下的并发安全 public synchronized V get() &#123; //再次检查一遍，这样第二个往后进来的线程就可以直接返回，得到缓存中的值 //避免重复计算 Supplier&lt;V&gt; supplier = valuesMap.get(subKey); if (supplier != this) &#123; return null; &#125; //现在supplier == this V value = null; try &#123; //这里产生新的代理类class value = Objects.requireNonNull(valueFactory.apply(key, parameter)); &#125; finally &#123; if (value == null) &#123; //如果value还是null，说明出现了失败 valuesMap.remove(subKey, this); &#125; &#125; //还是检查一遍value不为空 assert value != null; //把代理类class包装进CacheValue中 CacheValue&lt;V&gt; cacheValue = new CacheValue&lt;&gt;(value); if (valuesMap.replace(subKey, this, cacheValue)) &#123; reverseMap.put(cacheValue, Boolean.TRUE); &#125; else &#123; throw new AssertionError(\"Should not reach here\"); &#125; return value; &#125;&#125; 那是怎么处理过期的缓存呢，就是被gc收集走的那些缓存 在WeakCache#get中还执行了 1234567891011121314151617181920212223242526272829expungeStaleEntries(); /** * 如果你还记得在WeakCache中有个对象是 * private final ReferenceQueue&lt;K&gt; refQueue = new ReferenceQueue&lt;&gt;(); * 因为弱引用在下一次gc后被回收，那么那些指向对象的引用，就会被放到这个queue中 */private void expungeStaleEntries() &#123; CacheKey&lt;K&gt; cacheKey; while ((cacheKey = (CacheKey&lt;K&gt;)refQueue.poll()) != null) &#123; cacheKey.expungeFrom(map, reverseMap); &#125; &#125;//CacheKey方法中，把自己从map中移除void expungeFrom(ConcurrentMap&lt;?, ? extends ConcurrentMap&lt;?, ?&gt;&gt; map, ConcurrentMap&lt;?, Boolean&gt; reverseMap) &#123; // removing just by key is always safe here because after a CacheKey // is cleared and enqueue-ed it is only equal to itself // (see equals method)... ConcurrentMap&lt;?, ?&gt; valuesMap = map.remove(this); // remove also from reverseMap if needed if (valuesMap != null) &#123; for (Object cacheValue : valuesMap.values()) &#123; reverseMap.remove(cacheValue); &#125; &#125; &#125; 至此，分析都差不多了，下面看看到底是怎么生成字节码的，在ProxyClassFactory的apply方法中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889@Overridepublic Class&lt;?&gt; apply(ClassLoader loader, Class&lt;?&gt;[] interfaces) &#123; Map&lt;Class&lt;?&gt;, Boolean&gt; interfaceSet = new IdentityHashMap&lt;&gt;(interfaces.length); for (Class&lt;?&gt; intf : interfaces) &#123; /* * Verify that the class loader resolves the name of this * interface to the same Class object. */ Class&lt;?&gt; interfaceClass = null; try &#123; interfaceClass = Class.forName(intf.getName(), false, loader); &#125; catch (ClassNotFoundException e) &#123; &#125; if (interfaceClass != intf) &#123; throw new IllegalArgumentException( intf + \" is not visible from class loader\"); &#125; /* * Verify that the Class object actually represents an * interface. */ if (!interfaceClass.isInterface()) &#123; throw new IllegalArgumentException( interfaceClass.getName() + \" is not an interface\"); &#125; /* * Verify that this interface is not a duplicate. */ if (interfaceSet.put(interfaceClass, Boolean.TRUE) != null) &#123; throw new IllegalArgumentException( \"repeated interface: \" + interfaceClass.getName()); &#125; &#125; String proxyPkg = null; // package to define proxy class in int accessFlags = Modifier.PUBLIC | Modifier.FINAL; /* * Record the package of a non-public proxy interface so that the * proxy class will be defined in the same package. Verify that * all non-public proxy interfaces are in the same package. */ for (Class&lt;?&gt; intf : interfaces) &#123; int flags = intf.getModifiers(); if (!Modifier.isPublic(flags)) &#123; accessFlags = Modifier.FINAL; String name = intf.getName(); int n = name.lastIndexOf('.'); String pkg = ((n == -1) ? \"\" : name.substring(0, n + 1)); if (proxyPkg == null) &#123; proxyPkg = pkg; &#125; else if (!pkg.equals(proxyPkg)) &#123; throw new IllegalArgumentException( \"non-public interfaces from different packages\"); &#125; &#125; &#125; if (proxyPkg == null) &#123; // if no non-public proxy interfaces, use com.sun.proxy package proxyPkg = ReflectUtil.PROXY_PACKAGE + \".\"; &#125; /* * Choose a name for the proxy class to generate. */ long num = nextUniqueNumber.getAndIncrement(); String proxyName = proxyPkg + proxyClassNamePrefix + num; /* * Generate the specified proxy class. */ byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); try &#123; return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); &#125; catch (ClassFormatError e) &#123; /* * A ClassFormatError here means that (barring bugs in the * proxy class generation code) there was some other * invalid aspect of the arguments supplied to the proxy * class creation (such as virtual machine limitations * exceeded). */ throw new IllegalArgumentException(e.toString()); &#125;&#125; 主要方法在ProxyGenerator#generateProxyClass方法中，返回的是一个二进制的class流byte[ ] 123456789101112131415161718192021222324252627282930public class ProxyGenerator &#123; public static byte[] generateProxyClass(final String var0, Class&lt;?&gt;[] var1, int var2) &#123; ProxyGenerator var3 = new ProxyGenerator(var0, var1, var2); final byte[] var4 = var3.generateClassFile(); if (saveGeneratedFiles) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; try &#123; int var1 = var0.lastIndexOf(46); Path var2; if (var1 &gt; 0) &#123; Path var3 = Paths.get(var0.substring(0, var1).replace('.', File.separatorChar)); Files.createDirectories(var3); var2 = var3.resolve(var0.substring(var1 + 1, var0.length()) + \".class\"); &#125; else &#123; var2 = Paths.get(var0 + \".class\"); &#125; Files.write(var2, var4, new OpenOption[0]); return null; &#125; catch (IOException var4x) &#123; throw new InternalError(\"I/O exception saving generated file: \" + var4x); &#125; &#125; &#125;); &#125; return var4; &#125;&#125; 下面就不进行赘述了。","link":"/2017/09/10/动态代理的缓存实现/"},{"title":"在ubunut16.04上搭建hadoop环境","text":"首先我们需要安装几个常用的软件这里的shell我选的是zsh，因为这个语法兼容bash 1234567891011121314151617181920212223242526272829303132333435# 建立一个hadoop用户useradd -d /home/zhuyichen -m -s /bin/zsh hadoop # 安装gitapt install git -y# 安装wget apt install wget -y# 安装htopapt install htop -y# 安装openjdk8apt install openjdk-8-jdk -y # 切换为hadoop用户su# 下载hadoop-2.9.1wget http://www-us.apache.org/dist/hadoop/common/hadoop-2.9.1/hadoop-2.9.1.tar.gz# 解压文件tar -xvf hadoop-2.9.1.tar.gz # 配置一下PATH环境变量，打开.zshrc后添加export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export PATH=$PATH:~hadoop/hadoop-2.9.1/binexport PATH=$PATH:~hadoop/hadoop-2.9.1/sbinexport HADOOP_HOME=~hadoop/hadoop-2.9.1 # hadoop-env.sh配置中还要单独配置一下JAVA_HOMEecho 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' &gt;&gt; ~hadoop/hadoop-2.9.1/etc/hadoop/hadoop-env.sh # 配置sshssh_keygen # 把自己的key加到authorized_keys文件中cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys 下面开始分布式环境的配置因为分布式需要多态机器可以把上面这个克隆一下，这样就避免了很多的重复配置 我现在有两台虚拟机ip分别为[ip:1]和[ip:2]([ip:1]和[ip:2]替换为真实ip，如192.168.168.3) 把[ip:1]作为master，[ip:2]作为slave1 首先修改/etc/hostname文件[ip:1]的机器改为master, [ip:2]的机器改为slave1 再在/etc/hosts中添加ip映射12[ip:1] master[ip:2] slave1 然后master的机器需要可以ssh进slave机器，所以还需要把master的.ssh/id_rsa.pub的内容加进slave1的.ssh/authorized_keys中。 下面进行配置文件的修改 在hadoop的etc的文件夹下配置一下文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!-- core-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; &lt;!-- hdfs-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/data/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/data/datanode&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; &lt;!-- mapred-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; &lt;!-- yarn-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 同时还需要建立/usr/local/hadoop目录 如果机器是master的话，还需要手动指定slaves在$HADOOP_HOME/etc/hadoop/slaves文件中加上slave1 下面就可以启动了123hdfs namenode -formatstart-dfs.shstart-yarn.sh 如果成功的话，jps命令在master上应该除了jps本身外还显示3个进程分别为123NameNodeSecondaryNameNodeResourceManager 在slave上应该还额外显示2两进程12DataNodeNodeManager 下面如果我们需要自己写一些java文件进行练手的话我推荐的模式是利用idea的远程文件的功能在本地编辑，然后上传到服务器运行一个脚本这样的话我们就可以利用强大的idea写程序同时又可以在服务器很方便的跑了 1234#!/bin/bashjavac -classpath /home/hadoop/hadoop-2.9.1/etc/hadoop:/home/hadoop/hadoop-2.9.1/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/common/*:/home/hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/*:/home/hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.9.1/contrib/capacity-scheduler/*.jar Main.javajar cvf main.jar Main.classhadoop jar main.jar Main $* 这样我们在idea本地编辑好Main.java文件，然后上传到服务器，在服务器执行./start.sh就可以执行了。","link":"/2018/04/01/在ubuntu16.04上搭建hadoop环境/"}],"tags":[{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Network","slug":"Network","link":"/tags/Network/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"Tomcat","slug":"Tomcat","link":"/tags/Tomcat/"},{"name":"pigeon","slug":"pigeon","link":"/tags/pigeon/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"Vim","slug":"Vim","link":"/tags/Vim/"},{"name":"Logger","slug":"Logger","link":"/tags/Logger/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"}],"categories":[]}